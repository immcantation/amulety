{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMULETY CLI Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to use AMULETY command line interface (CLI) to translate and embed both BCR (B-cell receptor) and TCR (T-cell receptor) sequences. AMULETY supports a wide range of embedding models for different immune receptor types.\n",
    "\n",
    "Before getting started, please install AMULETY using `pip install amulety`. You can check available commands from AMULETY by running the help command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mamulety [OPTIONS] COMMAND [ARGS]...\u001b[0m\u001b[1m                                    \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-install\u001b[0m\u001b[1;36m-completion\u001b[0m          Install completion for the current shell.      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-show\u001b[0m\u001b[1;36m-completion\u001b[0m             Show completion for the current shell, to copy \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                               it or customize the installation.              \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                        Show this message and exit.                    \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Commands \u001b[0m\u001b[2m──────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mtranslate-igblast \u001b[0m\u001b[1;36m \u001b[0m Translates nucleotide sequences to amino acid sequences  \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m                   \u001b[0m using IgBlast.                                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36membed             \u001b[0m\u001b[1;36m \u001b[0m Embeds sequences from an AIRR rearrangement file using   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m                   \u001b[0m the specified model. Example usage:                      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mcheck-deps        \u001b[0m\u001b[1;36m \u001b[0m Check if optional embedding dependencies and tools are   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m                   \u001b[0m installed.                                               \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If AMULETY is installed via pip\n",
    "! amulety --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Commands\n",
    "\n",
    "AMULETY provides three main commands:\n",
    "\n",
    "1. **`translate-igblast`** - Translates nucleotide sequences to amino acid sequences using IgBlast\n",
    "2. **`embed`** - Embeds sequences using various models (BCR, TCR, and protein language models)\n",
    "3. **`check-deps`** - Check if optional embedding dependencies are installed\n",
    "\n",
    "### Supported Models\n",
    "\n",
    "AMULETY supports multiple categories of embedding models:\n",
    "\n",
    "**BCR Models:**\n",
    "- `ablang` - AbLang model for antibody sequences\n",
    "- `antiberta2` - AntiBERTa2 RoFormer model\n",
    "- `antiberty` - AntiBERTy model\n",
    "- `balm-paired` - BALM-paired model for heavy-light chain pairs\n",
    "\n",
    "**TCR Models:**\n",
    "- `tcr-bert` - TCR-BERT model for T-cell receptors\n",
    "- `tcrt5` - TCRT5 model (beta chains only)\n",
    "\n",
    "**Immune Models (BCR & TCR):**\n",
    "- `immune2vec` - Immune2Vec model for both BCR and TCR\n",
    "\n",
    "**Protein Language Models:**\n",
    "- `esm2` - ESM2 protein language model\n",
    "- `prott5` - ProtT5 protein language model\n",
    "- `custom` - Custom/fine-tuned models from HuggingFace\n",
    "\n",
    "### Chain Types\n",
    "\n",
    "AMULETY supports different chain input formats:\n",
    "- **H** - Heavy chain (BCR) or Beta/Delta chain (TCR)\n",
    "- **L** - Light chain (BCR) or Alpha/Gamma chain (TCR)\n",
    "- **HL** - Heavy-Light paired chains (BCR) or Beta-Alpha/Delta-Gamma paired chains (TCR)\n",
    "- **LH** - Light-Heavy paired chains (BCR) or Alpha-Beta/Gamma-Delta paired chains (TCR)\n",
    "- **H+L** - Both chains separately (not paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating nucleotides to amino acid sequences\n",
    "\n",
    "The inputs to the embedding models are [AIRR format files](https://docs.airr-community.org/en/stable/datarep/overview.html#datarepresentations) with immune receptor amino acid sequences. If the AIRR file only contains nucleotide sequences, the `amulety translate-igblast` command can help with the translation. The input requires:\n",
    "\n",
    "- Path to the V(D)J sequence AIRR file\n",
    "- Output directory path to write the translated sequences\n",
    "- Reference IgBlast database to perform alignment and translation\n",
    "\n",
    "### Download BCR example data and reference database\n",
    "The following command downloads an example AIRR format file of BCR sequences and the reference IgBlast database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create tutorial directory and download example data\n",
    "mkdir -p tutorial\n",
    "wget -P tutorial https://zenodo.org/records/11373741/files/AIRR_subject1_FNA_d0_1_Y1.tsv\n",
    "\n",
    "# Download and extract IgBlast reference database\n",
    "wget -P tutorial -c https://github.com/nf-core/test-datasets/raw/airrflow/database-cache/igblast_base.zip\n",
    "unzip tutorial/igblast_base.zip -d tutorial\n",
    "rm tutorial/igblast_base.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the translation command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-08-20 13:40:26,220 - INFO - Converting AIRR table to FastA for IgBlast translation...\n",
      "2025-08-20 13:40:26,224 - INFO - Calling IgBlast for running translation...\n",
      "2025-08-20 13:40:27,860 - INFO - Saved the translations in the dataframe (sequence_aa contains the full translation and sequence_vdj_aa contains the VDJ translation).\n",
      "2025-08-20 13:40:27,864 - INFO - Took 1.64 seconds\n",
      "2025-08-20 13:40:27,864 - INFO - Saved the translations in ../tutorial/output/AIRR_subject1_FNA_d0_1_Y1_translated.tsv file.\n"
     ]
    }
   ],
   "source": [
    "! amulety translate-igblast ../tutorial/AIRR_subject1_FNA_d0_1_Y1.tsv ../tutorial/output ../tutorial/igblast_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding sequences\n",
    "\n",
    "Now we are ready to embed the sequences using various models. AMULETY uses a unified `embed` command that supports all available models.\n",
    "\n",
    "### Basic usage\n",
    "\n",
    "The basic syntax for the embed command is:\n",
    "\n",
    "```bash\n",
    "amulety embed --input-airr [INPUT_FILE] --chain [CHAIN] --model [MODEL] --batch-size [BATCH_SIZE] --output-file-path [OUTPUT]\n",
    "```\n",
    "\n",
    "### Required arguments:\n",
    "\n",
    "* `--chain`: Chain(s) to embed\n",
    "  - For BCR: `H` (Heavy), `L` (Light), `HL` (Heavy-Light pairs), `LH` (Light-Heavy pairs), `H+L` (Both chains separately)\n",
    "  - For TCR: `H` (Beta/Delta), `L` (Alpha/Gamma), `HL` (Beta-Alpha/Delta-Gamma pairs), `LH` (Alpha-Beta/Gamma-Delta pairs), `H+L` (Both chains separately)\n",
    "\n",
    "* `--model`: The embedding model to use (see model list above)\n",
    "\n",
    "* `--output-file-path`: Path to save embeddings (supports `.pt`, `.csv`, `.tsv` extensions)\n",
    "\n",
    "* `input_file`: Path to the input AIRR file\n",
    "\n",
    "### Optional arguments:\n",
    "\n",
    "* `--sequence-col`: Column containing amino acid sequences (default: `sequence_vdj_aa`)\n",
    "* `--cell-id-col`: Column containing single-cell barcodes (default: `cell_id`)\n",
    "* `--batch-size`: Mini-batch size for processing (default: 50)\n",
    "* `--cache-dir`: Directory for caching model weights (default: `/tmp/amulety`)\n",
    "* `--duplicate-col`: Column for selecting best chain when multiple exist (default: `duplicate_count`)\n",
    "\n",
    "### Custom model arguments (for `--model custom`):\n",
    "\n",
    "* `--model-path`: HuggingFace model name or local path\n",
    "* `--embedding-dimension`: Embedding dimension\n",
    "* `--max-length`: Maximum sequence length\n",
    "\n",
    "### Output formats:\n",
    "\n",
    "- `.pt` files: PyTorch tensors saved with `torch.save()` (embeddings only)\n",
    "- `.csv/.tsv` files: Include cell barcodes/sequence IDs as indices with embeddings\n",
    "\n",
    "The package automatically detects and uses GPU when available. Adjust `--batch-size` to avoid GPU out-of-memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCR embedding examples\n",
    "\n",
    "Let's demonstrate embedding BCR sequences using different models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AntiBERTy (BCR-specific model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-08-20 11:51:53,375 - INFO - Detected single-cell data format\n",
      "2025-08-20 11:51:53,376 - INFO - Processing single-cell data...\n",
      "2025-08-20 11:51:53,438 - INFO - AntiBERTy loaded. Size: 26.03 M\n",
      "2025-08-20 11:51:53,438 - INFO - Batch 1/48\n",
      "2025-08-20 11:51:53,475 - INFO - Batch 2/48\n",
      "2025-08-20 11:51:53,502 - INFO - Batch 3/48\n",
      "2025-08-20 11:51:53,527 - INFO - Batch 4/48\n",
      "2025-08-20 11:51:53,553 - INFO - Batch 5/48\n",
      "2025-08-20 11:51:53,579 - INFO - Batch 6/48\n",
      "2025-08-20 11:51:53,606 - INFO - Batch 7/48\n",
      "2025-08-20 11:51:53,633 - INFO - Batch 8/48\n",
      "2025-08-20 11:51:53,658 - INFO - Batch 9/48\n",
      "2025-08-20 11:51:53,683 - INFO - Batch 10/48\n",
      "2025-08-20 11:51:53,708 - INFO - Batch 11/48\n",
      "2025-08-20 11:51:53,734 - INFO - Batch 12/48\n",
      "2025-08-20 11:51:53,759 - INFO - Batch 13/48\n",
      "2025-08-20 11:51:53,787 - INFO - Batch 14/48\n",
      "2025-08-20 11:51:53,813 - INFO - Batch 15/48\n",
      "2025-08-20 11:51:53,839 - INFO - Batch 16/48\n",
      "2025-08-20 11:51:53,866 - INFO - Batch 17/48\n",
      "2025-08-20 11:51:53,893 - INFO - Batch 18/48\n",
      "2025-08-20 11:51:53,918 - INFO - Batch 19/48\n",
      "2025-08-20 11:51:53,944 - INFO - Batch 20/48\n",
      "2025-08-20 11:51:53,970 - INFO - Batch 21/48\n",
      "2025-08-20 11:51:53,995 - INFO - Batch 22/48\n",
      "2025-08-20 11:51:54,021 - INFO - Batch 23/48\n",
      "2025-08-20 11:51:54,046 - INFO - Batch 24/48\n",
      "2025-08-20 11:51:54,072 - INFO - Batch 25/48\n",
      "2025-08-20 11:51:54,098 - INFO - Batch 26/48\n",
      "2025-08-20 11:51:54,124 - INFO - Batch 27/48\n",
      "2025-08-20 11:51:54,156 - INFO - Batch 28/48\n",
      "2025-08-20 11:51:54,181 - INFO - Batch 29/48\n",
      "2025-08-20 11:51:54,207 - INFO - Batch 30/48\n",
      "2025-08-20 11:51:54,234 - INFO - Batch 31/48\n",
      "2025-08-20 11:51:54,260 - INFO - Batch 32/48\n",
      "2025-08-20 11:51:54,285 - INFO - Batch 33/48\n",
      "2025-08-20 11:51:54,310 - INFO - Batch 34/48\n",
      "2025-08-20 11:51:54,335 - INFO - Batch 35/48\n",
      "2025-08-20 11:51:54,360 - INFO - Batch 36/48\n",
      "2025-08-20 11:51:54,386 - INFO - Batch 37/48\n",
      "2025-08-20 11:51:54,411 - INFO - Batch 38/48\n",
      "2025-08-20 11:51:54,436 - INFO - Batch 39/48\n",
      "2025-08-20 11:51:54,462 - INFO - Batch 40/48\n",
      "2025-08-20 11:51:54,489 - INFO - Batch 41/48\n",
      "2025-08-20 11:51:54,514 - INFO - Batch 42/48\n",
      "2025-08-20 11:51:54,540 - INFO - Batch 43/48\n",
      "2025-08-20 11:51:54,567 - INFO - Batch 44/48\n",
      "2025-08-20 11:51:54,593 - INFO - Batch 45/48\n",
      "2025-08-20 11:51:54,619 - INFO - Batch 46/48\n",
      "2025-08-20 11:51:54,647 - INFO - Batch 47/48\n",
      "2025-08-20 11:51:54,676 - INFO - Batch 48/48\n",
      "2025-08-20 11:51:54,694 - INFO - Took 1.26 seconds\n",
      "2025-08-20 11:51:54,697 - INFO - Saved embedding at ../tutorial/test_embedding.pt\n"
     ]
    }
   ],
   "source": [
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model antiberty --batch-size 2 --output-file-path ../tutorial/test_embedding.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AntiBERTa2 (BCR-specific model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-08-21 11:55:10,483 - INFO - Detected single-cell data format\n",
      "2025-08-21 11:55:10,484 - INFO - Processing single-cell data...\n",
      "RoFormerForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "RoFormerForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "RoFormerForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "2025-08-21 11:55:10,970 - INFO - AntiBERTa2 loaded. Size: 202.642462 M\n",
      "2025-08-21 11:55:10,970 - INFO - Batch 1/48.\n",
      "2025-08-21 11:55:13,036 - INFO - Batch 2/48.\n",
      "2025-08-21 11:55:13,306 - INFO - Batch 3/48.\n",
      "2025-08-21 11:55:13,567 - INFO - Batch 4/48.\n",
      "2025-08-21 11:55:13,844 - INFO - Batch 5/48.\n",
      "2025-08-21 11:55:14,118 - INFO - Batch 6/48.\n",
      "2025-08-21 11:55:14,414 - INFO - Batch 7/48.\n",
      "2025-08-21 11:55:14,687 - INFO - Batch 8/48.\n",
      "2025-08-21 11:55:14,970 - INFO - Batch 9/48.\n",
      "2025-08-21 11:55:15,253 - INFO - Batch 10/48.\n",
      "2025-08-21 11:55:15,516 - INFO - Batch 11/48.\n",
      "2025-08-21 11:55:15,779 - INFO - Batch 12/48.\n",
      "2025-08-21 11:55:16,022 - INFO - Batch 13/48.\n",
      "2025-08-21 11:55:16,251 - INFO - Batch 14/48.\n",
      "2025-08-21 11:55:16,480 - INFO - Batch 15/48.\n",
      "2025-08-21 11:55:16,709 - INFO - Batch 16/48.\n",
      "2025-08-21 11:55:16,940 - INFO - Batch 17/48.\n",
      "2025-08-21 11:55:17,180 - INFO - Batch 18/48.\n",
      "2025-08-21 11:55:17,409 - INFO - Batch 19/48.\n",
      "2025-08-21 11:55:17,639 - INFO - Batch 20/48.\n",
      "2025-08-21 11:55:17,873 - INFO - Batch 21/48.\n",
      "2025-08-21 11:55:18,105 - INFO - Batch 22/48.\n",
      "2025-08-21 11:55:18,334 - INFO - Batch 23/48.\n",
      "2025-08-21 11:55:18,577 - INFO - Batch 24/48.\n",
      "2025-08-21 11:55:18,807 - INFO - Batch 25/48.\n",
      "2025-08-21 11:55:19,039 - INFO - Batch 26/48.\n",
      "2025-08-21 11:55:19,275 - INFO - Batch 27/48.\n",
      "2025-08-21 11:55:19,502 - INFO - Batch 28/48.\n",
      "2025-08-21 11:55:19,734 - INFO - Batch 29/48.\n",
      "2025-08-21 11:55:19,965 - INFO - Batch 30/48.\n",
      "2025-08-21 11:55:20,199 - INFO - Batch 31/48.\n",
      "2025-08-21 11:55:20,435 - INFO - Batch 32/48.\n",
      "2025-08-21 11:55:20,670 - INFO - Batch 33/48.\n",
      "2025-08-21 11:55:20,901 - INFO - Batch 34/48.\n",
      "2025-08-21 11:55:21,133 - INFO - Batch 35/48.\n",
      "2025-08-21 11:55:21,364 - INFO - Batch 36/48.\n",
      "2025-08-21 11:55:21,595 - INFO - Batch 37/48.\n",
      "2025-08-21 11:55:21,829 - INFO - Batch 38/48.\n",
      "2025-08-21 11:55:22,062 - INFO - Batch 39/48.\n",
      "2025-08-21 11:55:22,292 - INFO - Batch 40/48.\n",
      "2025-08-21 11:55:22,529 - INFO - Batch 41/48.\n",
      "2025-08-21 11:55:22,759 - INFO - Batch 42/48.\n",
      "2025-08-21 11:55:22,992 - INFO - Batch 43/48.\n",
      "2025-08-21 11:55:23,222 - INFO - Batch 44/48.\n",
      "2025-08-21 11:55:23,458 - INFO - Batch 45/48.\n",
      "2025-08-21 11:55:23,695 - INFO - Batch 46/48.\n",
      "2025-08-21 11:55:23,932 - INFO - Batch 47/48.\n",
      "2025-08-21 11:55:24,166 - INFO - Batch 48/48.\n",
      "2025-08-21 11:55:24,307 - INFO - Took 13.34 seconds\n",
      "2025-08-21 11:55:24,315 - INFO - Saved embedding at ../tutorial/AIRR_subject1_FNA_d0_1_Y1_antiberta2.pt\n"
     ]
    }
   ],
   "source": [
    "# Embed heavy-light chain pairs using AntiBERTa2\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model antiberta2 --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_antiberta2.pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AbLang (BCR-specific model with separate heavy/light models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed both heavy and light chains separately using AbLang\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H+L --model ablang --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_ablang.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BALM-paired model (BCR paired chains)\n",
    "\n",
    "BALM-paired is a specialized model for BCR heavy-light chain pairs. It automatically downloads the model weights when first used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-06 14:54:13--  https://zenodo.org/records/8237396/files/BALM-paired.tar.gz\n",
      "Resolving zenodo.org (zenodo.org)... 188.184.103.159, 188.184.98.238, 188.185.79.172, ...\n",
      "Connecting to zenodo.org (zenodo.org)|188.184.103.159|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1129993036 (1.1G) [application/octet-stream]\n",
      "Saving to: ‘tutorial/BALM-paired.tar.gz.1’\n",
      "\n",
      "BALM-paired.tar.gz. 100%[===================>]   1.05G  37.8MB/s    in 26s     \n",
      "\n",
      "2024-06-06 14:54:40 (41.3 MB/s) - ‘tutorial/BALM-paired.tar.gz.1’ saved [1129993036/1129993036]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget -P tutorial https://zenodo.org/records/8237396/files/BALM-paired.tar.gz\n",
    "tar -xzf tutorial/BALM-paired.tar.gz -C tutorial\n",
    "rm tutorial/BALM-paired.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the parameters mentioned above, we need to specify the following parameters:\n",
    "\n",
    "* `modelpath`: the path to the downloaded model weights\n",
    "\n",
    "* `embedding-dimension`: the dimension of the embedding\n",
    "\n",
    "* `max-length`: maximum length taken by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m1.0\u001b[0m\n",
      "\n",
      "2024-06-06 15:21:05,068 - INFO - Processing single-cell BCR data...\n",
      "2024-06-06 15:21:05,068 - INFO - Concatenating heavy and light chain per cell...\n",
      "2024-06-06 15:21:07,869 - INFO - Model size: 303.92M\n",
      "Batch 1/4\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/mw957/.conda/envs/bcrembed/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Batch 2/4\n",
      "\n",
      "Batch 3/4\n",
      "\n",
      "Batch 4/4\n",
      "\n",
      "2024-06-06 15:28:50,302 - INFO - Took 462.43 seconds\n",
      "2024-06-06 15:28:50,423 - INFO - Saved embedding at tutorial/AIRR_subject1_FNA_d0_1_Y1_BALM-paired.tsv\n"
     ]
    }
   ],
   "source": [
    "# Embed heavy-light chain pairs using BALM-paired\n",
    "# The model will be automatically downloaded on first use\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain HL --model balm-paired --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_balm_paired.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein Language Models\n",
    "\n",
    "Then we want to use the same dataset to embed using the general protein language models.\n",
    "\n",
    "#### ESM2 (Protein language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-09-08 22:52:48,775 - INFO - Detected single-cell data format\n",
      "2025-09-08 22:52:48,777 - INFO - Processing both BCR and TCR sequences from the file.\n",
      "2025-09-08 22:52:48,777 - INFO - Single-cell AIRR data detected (all entries have cell_id).\n",
      "2025-09-08 22:52:48,778 - INFO - Removed 102 sequences not matching H chain\n",
      "tokenizer_config.json: 100%|██████████████████| 95.0/95.0 [00:00<00:00, 281kB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 93.0/93.0 [00:00<00:00, 1.31MB/s]\n",
      "special_tokens_map.json: 100%|██████████████████| 125/125 [00:00<00:00, 592kB/s]\n",
      "config.json: 100%|█████████████████████████████| 724/724 [00:00<00:00, 5.54MB/s]\n",
      "model.safetensors: 100%|████████████████████| 2.61G/2.61G [00:04<00:00, 600MB/s]\n",
      "Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForMaskedLM: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-09-08 22:52:54,414 - INFO - ESM2 650M model size: 651.04 M\n",
      "2025-09-08 22:52:54,415 - INFO - Batch 1/48.\n",
      "2025-09-08 22:53:01,255 - INFO - Batch 2/48.\n",
      "2025-09-08 22:53:02,767 - INFO - Batch 3/48.\n",
      "2025-09-08 22:53:04,139 - INFO - Batch 4/48.\n",
      "2025-09-08 22:53:05,532 - INFO - Batch 5/48.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Embed heavy chains only using ESM2\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model esm2 --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_esm2.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immune2Vec \n",
    "Immune2Vec requires manual installation follows by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Installing Immune2Vec\n",
    "# Clone repository\n",
    "git clone https://bitbucket.org/yaarilab/immune2vec_model.git\n",
    "\n",
    "# please store the path: /path/to/immune2vec_model for later use:\n",
    "# using custom path\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model immune2vec --immune2vec-path /path/to/immune2vec_model --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_immune2vec.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom/Fine-tuned models\n",
    "\n",
    "You can use custom or fine-tuned models from HuggingFace or local paths using the `custom` model type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-09-08 23:12:04,178 - INFO - Detected single-cell data format\n",
      "2025-09-08 23:12:04,179 - INFO - Processing both BCR and TCR sequences from the file.\n",
      "2025-09-08 23:12:04,179 - INFO - Single-cell AIRR data detected (all entries have cell_id).\n",
      "2025-09-08 23:12:04,180 - INFO - Removed 102 sequences not matching H chain\n",
      "tokenizer_config.json: 100%|████████████████████| 108/108 [00:00<00:00, 247kB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 93.0/93.0 [00:00<00:00, 1.22MB/s]\n",
      "special_tokens_map.json: 100%|██████████████████| 125/125 [00:00<00:00, 599kB/s]\n",
      "config.json: 100%|█████████████████████████████| 828/828 [00:00<00:00, 2.56MB/s]\n",
      "model.safetensors: 100%|███████████████████| 31.4M/31.4M [00:02<00:00, 15.6MB/s]\n",
      "Some weights of EsmForMaskedLM were not initialized from the model checkpoint at AmelieSchreiber/esm2_t6_8M_UR50D-finetuned-localization and are newly initialized: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-09-08 23:12:07,508 - INFO - Model size: 7.51M\n",
      "Batch 1/48\n",
      "\n",
      "Batch 2/48\n",
      "\n",
      "Batch 3/48\n",
      "\n",
      "Batch 4/48\n",
      "\n",
      "Batch 5/48\n",
      "\n",
      "Batch 6/48\n",
      "\n",
      "Batch 7/48\n",
      "\n",
      "Batch 8/48\n",
      "\n",
      "Batch 9/48\n",
      "\n",
      "Batch 10/48\n",
      "\n",
      "Batch 11/48\n",
      "\n",
      "Batch 12/48\n",
      "\n",
      "Batch 13/48\n",
      "\n",
      "Batch 14/48\n",
      "\n",
      "Batch 15/48\n",
      "\n",
      "Batch 16/48\n",
      "\n",
      "Batch 17/48\n",
      "\n",
      "Batch 18/48\n",
      "\n",
      "Batch 19/48\n",
      "\n",
      "Batch 20/48\n",
      "\n",
      "Batch 21/48\n",
      "\n",
      "Batch 22/48\n",
      "\n",
      "Batch 23/48\n",
      "\n",
      "Batch 24/48\n",
      "\n",
      "Batch 25/48\n",
      "\n",
      "Batch 26/48\n",
      "\n",
      "Batch 27/48\n",
      "\n",
      "Batch 28/48\n",
      "\n",
      "Batch 29/48\n",
      "\n",
      "Batch 30/48\n",
      "\n",
      "Batch 31/48\n",
      "\n",
      "Batch 32/48\n",
      "\n",
      "Batch 33/48\n",
      "\n",
      "Batch 34/48\n",
      "\n",
      "Batch 35/48\n",
      "\n",
      "Batch 36/48\n",
      "\n",
      "Batch 37/48\n",
      "\n",
      "Batch 38/48\n",
      "\n",
      "Batch 39/48\n",
      "\n",
      "Batch 40/48\n",
      "\n",
      "Batch 41/48\n",
      "\n",
      "Batch 42/48\n",
      "\n",
      "Batch 43/48\n",
      "\n",
      "Batch 44/48\n",
      "\n",
      "Batch 45/48\n",
      "\n",
      "Batch 46/48\n",
      "\n",
      "Batch 47/48\n",
      "\n",
      "Batch 48/48\n",
      "\n",
      "2025-09-08 23:12:12,318 - INFO - Took 4.81 seconds\n",
      "2025-09-08 23:12:12,322 - INFO - Saved embedding at ../tutorial/custom_model_embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "# Example: Using a fine-tuned ESM2 model from HuggingFace\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model custom \\\n",
    "  --model-path \"AmelieSchreiber/esm2_t6_8M_UR50D-finetuned-localization\" \\\n",
    "  --embedding-dimension 320 \\\n",
    "  --max-length 512 \\\n",
    "  --batch-size 2 \\\n",
    "  --output-file-path ../tutorial/custom_model_embeddings.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TCR embedding examples\n",
    "\n",
    "AMULETY also supports TCR-specific models. Here we also provide TCR example data and you can download and have a try: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tutorial directory and download TCR example data\n",
    "# TBD...\n",
    "wget -P tutorial https://zenodo.org/records/11373741/TBD..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCR-BERT (TCR-specific model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-09-06 15:41:06,325 - INFO - Detected single-cell data format\n",
      "2025-09-06 15:41:06,326 - INFO - Single-cell AIRR data detected (all entries have cell_id).\n",
      "2025-09-06 15:41:06,330 - INFO - Loading TCR-BERT model for TCR embedding...\n",
      "2025-09-06 15:41:06,937 - INFO - Successfully loaded TCR-BERT model\n",
      "2025-09-06 15:41:06,937 - INFO - TCR-BERT model loaded. Size: 57.39 M\n",
      "2025-09-06 15:41:06,937 - INFO - TCR-BERT Batch 1/25.\n",
      "2025-09-06 15:41:06,989 - INFO - TCR-BERT Batch 2/25.\n",
      "2025-09-06 15:41:07,022 - INFO - TCR-BERT Batch 3/25.\n",
      "2025-09-06 15:41:07,052 - INFO - TCR-BERT Batch 4/25.\n",
      "2025-09-06 15:41:07,082 - INFO - TCR-BERT Batch 5/25.\n",
      "2025-09-06 15:41:07,113 - INFO - TCR-BERT Batch 6/25.\n",
      "2025-09-06 15:41:07,142 - INFO - TCR-BERT Batch 7/25.\n",
      "2025-09-06 15:41:07,173 - INFO - TCR-BERT Batch 8/25.\n",
      "2025-09-06 15:41:07,204 - INFO - TCR-BERT Batch 9/25.\n",
      "2025-09-06 15:41:07,237 - INFO - TCR-BERT Batch 10/25.\n",
      "2025-09-06 15:41:07,266 - INFO - TCR-BERT Batch 11/25.\n",
      "2025-09-06 15:41:07,294 - INFO - TCR-BERT Batch 12/25.\n",
      "2025-09-06 15:41:07,324 - INFO - TCR-BERT Batch 13/25.\n",
      "2025-09-06 15:41:07,355 - INFO - TCR-BERT Batch 14/25.\n",
      "2025-09-06 15:41:07,385 - INFO - TCR-BERT Batch 15/25.\n",
      "2025-09-06 15:41:07,414 - INFO - TCR-BERT Batch 16/25.\n",
      "2025-09-06 15:41:07,443 - INFO - TCR-BERT Batch 17/25.\n",
      "2025-09-06 15:41:07,473 - INFO - TCR-BERT Batch 18/25.\n",
      "2025-09-06 15:41:07,502 - INFO - TCR-BERT Batch 19/25.\n",
      "2025-09-06 15:41:07,532 - INFO - TCR-BERT Batch 20/25.\n",
      "2025-09-06 15:41:07,561 - INFO - TCR-BERT Batch 21/25.\n",
      "2025-09-06 15:41:07,591 - INFO - TCR-BERT Batch 22/25.\n",
      "2025-09-06 15:41:07,620 - INFO - TCR-BERT Batch 23/25.\n",
      "2025-09-06 15:41:07,648 - INFO - TCR-BERT Batch 24/25.\n",
      "2025-09-06 15:41:07,677 - INFO - TCR-BERT Batch 25/25.\n",
      "2025-09-06 15:41:07,706 - INFO - TCR-BERT embedding took 0.77 seconds\n",
      "2025-09-06 15:41:07,709 - INFO - Saved embedding at ../tutorial/tcr_embeddings_tcrbert.pt\n"
     ]
    }
   ],
   "source": [
    "# Embed TCR beta-alpha chain pairs using TCR-BERT\n",
    "# Note: This assumes you have TCR data in AIRR format\n",
    "! amulety embed --input-airr ../tutorial/AIRR_tcr_sample.tsv --chain HL --model tcr-bert --batch-size 2 --output-file-path ../tutorial/tcr_embeddings_tcrbert.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCRT5 (TCR beta chain only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-09-06 15:40:29,129 - INFO - Detected single-cell data format\n",
      "2025-09-06 15:40:29,131 - INFO - Single-cell AIRR data detected (all entries have cell_id).\n",
      "2025-09-06 15:40:29,131 - INFO - Removed 100 sequences not matching H chain\n",
      "2025-09-06 15:40:29,133 - INFO - Loading TCRT5 model for TCR embedding...\n",
      "tokenizer_config.json: 21.1kB [00:00, 6.70MB/s]\n",
      "spiece.model: 100%|██████████████████████████| 238k/238k [00:00<00:00, 2.87MB/s]\n",
      "added_tokens.json: 2.35kB [00:00, 10.1MB/s]\n",
      "special_tokens_map.json: 2.64kB [00:00, 9.78MB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'TCRT5Tokenizer'. \n",
      "The class this function is called from is 'T5Tokenizer'.\n",
      "config.json: 100%|█████████████████████████████| 970/970 [00:00<00:00, 7.69MB/s]\n",
      "model.safetensors: 100%|█████████████████████| 168M/168M [00:03<00:00, 45.7MB/s]\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states', 'output_scores']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states', 'output_scores']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "generation_config.json: 100%|███████████████████| 249/249 [00:00<00:00, 566kB/s]\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states', 'output_scores']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions', 'output_hidden_states', 'output_scores']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "2025-09-06 15:40:45,423 - INFO - TCRT5 Batch 1/50.\n",
      "2025-09-06 15:40:45,458 - INFO - TCRT5 Batch 2/50.\n",
      "2025-09-06 15:40:45,470 - INFO - TCRT5 Batch 3/50.\n",
      "2025-09-06 15:40:45,483 - INFO - TCRT5 Batch 4/50.\n",
      "2025-09-06 15:40:45,497 - INFO - TCRT5 Batch 5/50.\n",
      "2025-09-06 15:40:45,511 - INFO - TCRT5 Batch 6/50.\n",
      "2025-09-06 15:40:45,523 - INFO - TCRT5 Batch 7/50.\n",
      "2025-09-06 15:40:45,535 - INFO - TCRT5 Batch 8/50.\n",
      "2025-09-06 15:40:45,548 - INFO - TCRT5 Batch 9/50.\n",
      "2025-09-06 15:40:45,561 - INFO - TCRT5 Batch 10/50.\n",
      "2025-09-06 15:40:45,574 - INFO - TCRT5 Batch 11/50.\n",
      "2025-09-06 15:40:45,586 - INFO - TCRT5 Batch 12/50.\n",
      "2025-09-06 15:40:45,599 - INFO - TCRT5 Batch 13/50.\n",
      "2025-09-06 15:40:45,612 - INFO - TCRT5 Batch 14/50.\n",
      "2025-09-06 15:40:45,623 - INFO - TCRT5 Batch 15/50.\n",
      "2025-09-06 15:40:45,635 - INFO - TCRT5 Batch 16/50.\n",
      "2025-09-06 15:40:45,647 - INFO - TCRT5 Batch 17/50.\n",
      "2025-09-06 15:40:45,660 - INFO - TCRT5 Batch 18/50.\n",
      "2025-09-06 15:40:45,672 - INFO - TCRT5 Batch 19/50.\n",
      "2025-09-06 15:40:45,684 - INFO - TCRT5 Batch 20/50.\n",
      "2025-09-06 15:40:45,696 - INFO - TCRT5 Batch 21/50.\n",
      "2025-09-06 15:40:45,709 - INFO - TCRT5 Batch 22/50.\n",
      "2025-09-06 15:40:45,721 - INFO - TCRT5 Batch 23/50.\n",
      "2025-09-06 15:40:45,733 - INFO - TCRT5 Batch 24/50.\n",
      "2025-09-06 15:40:45,746 - INFO - TCRT5 Batch 25/50.\n",
      "2025-09-06 15:40:45,758 - INFO - TCRT5 Batch 26/50.\n",
      "2025-09-06 15:40:45,771 - INFO - TCRT5 Batch 27/50.\n",
      "2025-09-06 15:40:45,783 - INFO - TCRT5 Batch 28/50.\n",
      "2025-09-06 15:40:45,795 - INFO - TCRT5 Batch 29/50.\n",
      "2025-09-06 15:40:45,807 - INFO - TCRT5 Batch 30/50.\n",
      "2025-09-06 15:40:45,819 - INFO - TCRT5 Batch 31/50.\n",
      "2025-09-06 15:40:45,832 - INFO - TCRT5 Batch 32/50.\n",
      "2025-09-06 15:40:45,844 - INFO - TCRT5 Batch 33/50.\n",
      "2025-09-06 15:40:45,856 - INFO - TCRT5 Batch 34/50.\n",
      "2025-09-06 15:40:45,869 - INFO - TCRT5 Batch 35/50.\n",
      "2025-09-06 15:40:45,881 - INFO - TCRT5 Batch 36/50.\n",
      "2025-09-06 15:40:45,893 - INFO - TCRT5 Batch 37/50.\n",
      "2025-09-06 15:40:45,906 - INFO - TCRT5 Batch 38/50.\n",
      "2025-09-06 15:40:45,918 - INFO - TCRT5 Batch 39/50.\n",
      "2025-09-06 15:40:45,930 - INFO - TCRT5 Batch 40/50.\n",
      "2025-09-06 15:40:45,942 - INFO - TCRT5 Batch 41/50.\n",
      "2025-09-06 15:40:45,954 - INFO - TCRT5 Batch 42/50.\n",
      "2025-09-06 15:40:45,967 - INFO - TCRT5 Batch 43/50.\n",
      "2025-09-06 15:40:45,978 - INFO - TCRT5 Batch 44/50.\n",
      "2025-09-06 15:40:45,991 - INFO - TCRT5 Batch 45/50.\n",
      "2025-09-06 15:40:46,003 - INFO - TCRT5 Batch 46/50.\n",
      "2025-09-06 15:40:46,016 - INFO - TCRT5 Batch 47/50.\n",
      "2025-09-06 15:40:46,029 - INFO - TCRT5 Batch 48/50.\n",
      "2025-09-06 15:40:46,043 - INFO - TCRT5 Batch 49/50.\n",
      "2025-09-06 15:40:46,057 - INFO - TCRT5 Batch 50/50.\n",
      "2025-09-06 15:40:46,072 - INFO - TCRT5 embedding took 16.94 seconds\n",
      "2025-09-06 15:40:46,076 - INFO - Saved embedding at ../tutorial/tcr_embeddings_tcrt5.pt\n"
     ]
    }
   ],
   "source": [
    "# Embed TCR beta chains using TCRT5 (only supports H/beta chains)\n",
    "! amulety embed --input-airr ../tutorial/AIRR_tcr_sample.tsv --chain H --model tcrt5 --batch-size 2 --output-file-path ../tutorial/tcr_embeddings_tcrt5.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking dependencies\n",
    "\n",
    "Some models require additional dependencies that are not installed by default. You can check which dependencies are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "Checking AMULETY dependencies...\n",
      "\n",
      "IgBlast (for translate-igblast command):\n",
      "  IgBlast (igblastn) is available\n",
      "\n",
      "Embedding model dependencies:\n",
      "2025-09-08 23:27:12,074 - INFO - Available models: AntiBERTy, AbLang, TCREMP, TCR-BERT, TCRT5, ESM2, ProtT5\n",
      "2025-09-08 23:27:12,074 - WARNING - Missing model dependencies: Immune2Vec\n",
      "  1 dependencies are missing.\n",
      "  AMULETY will raise ImportError with installation instructions when these models are used.\n",
      "\n",
      "  To install missing dependencies:\n",
      "    • Immune2Vec: git clone https://bitbucket.org/yaarilab/immune2vec_model.git && add to Python path\n",
      "\n",
      "  Note: Models will provide detailed installation instructions when used.\n"
     ]
    }
   ],
   "source": [
    "# Check which optional dependencies are missing\n",
    "! amulety check-deps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amulety_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
