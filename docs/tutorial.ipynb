{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMULETY CLI Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to use AMULETY command line interface (CLI) to translate and embed both BCR (B-cell receptor) and TCR (T-cell receptor) sequences. AMULETY supports a wide range of embedding models for different immune receptor types.\n",
    "\n",
    "Before getting started, please install AMULETY using `pip install amulety`. You can check available commands from AMULETY by running the help command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mamulety [OPTIONS] COMMAND [ARGS]...\u001b[0m\u001b[1m                                    \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-install\u001b[0m\u001b[1;36m-completion\u001b[0m          Install completion for the current shell.      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-show\u001b[0m\u001b[1;36m-completion\u001b[0m             Show completion for the current shell, to copy \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                               it or customize the installation.              \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                        Show this message and exit.                    \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Commands \u001b[0m\u001b[2m──────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mtranslate-igblast \u001b[0m\u001b[1;36m \u001b[0m Translates nucleotide sequences to amino acid sequences  \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m                   \u001b[0m using IgBlast.                                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36membed             \u001b[0m\u001b[1;36m \u001b[0m Embeds sequences from an AIRR rearrangement file using   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m                   \u001b[0m the specified model. Example usage:                      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mcheck-deps        \u001b[0m\u001b[1;36m \u001b[0m Check if optional embedding dependencies and tools are   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m                   \u001b[0m installed.                                               \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If AMULETY is installed via pip\n",
    "! amulety --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Commands\n",
    "\n",
    "AMULETY provides three main commands:\n",
    "\n",
    "1. **`translate-igblast`** - Translates nucleotide sequences to amino acid sequences using IgBlast\n",
    "2. **`embed`** - Embeds sequences using various models (BCR, TCR, and protein language models)\n",
    "3. **`check-deps`** - Check if optional embedding dependencies are installed\n",
    "\n",
    "### Supported Models\n",
    "\n",
    "AMULETY supports multiple categories of embedding models:\n",
    "\n",
    "**BCR Models:**\n",
    "- `ablang` - AbLang model for antibody sequences\n",
    "- `antiberta2` - AntiBERTa2 RoFormer model\n",
    "- `antiberty` - AntiBERTy model\n",
    "- `balm-paired` - BALM-paired model for heavy-light chain pairs\n",
    "\n",
    "**TCR Models:**\n",
    "- `tcr-bert` - TCR-BERT model for T-cell receptors\n",
    "- `tcrt5` - TCRT5 model (beta chains only)\n",
    "\n",
    "**Immune Models (BCR & TCR):**\n",
    "- `immune2vec` - Immune2Vec model for both BCR and TCR\n",
    "\n",
    "**Protein Language Models:**\n",
    "- `esm2` - ESM2 protein language model\n",
    "- `prott5` - ProtT5 protein language model\n",
    "- `custom` - Custom/fine-tuned models from HuggingFace\n",
    "\n",
    "### Chain Types\n",
    "\n",
    "AMULETY supports different chain input formats:\n",
    "- **H** - Heavy chain (BCR) or Beta/Delta chain (TCR)\n",
    "- **L** - Light chain (BCR) or Alpha/Gamma chain (TCR)\n",
    "- **HL** - Heavy-Light paired chains (BCR) or Beta-Alpha/Delta-Gamma paired chains (TCR)\n",
    "- **LH** - Light-Heavy paired chains (BCR) or Alpha-Beta/Gamma-Delta paired chains (TCR)\n",
    "- **H+L** - Both chains separately (not paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating nucleotides to amino acid sequences\n",
    "\n",
    "The inputs to the embedding models are [AIRR format files](https://docs.airr-community.org/en/stable/datarep/overview.html#datarepresentations) with immune receptor amino acid sequences. If the AIRR file only contains nucleotide sequences, the `amulety translate-igblast` command can help with the translation.\n",
    "\n",
    "### Requirements for translation:\n",
    "- Path to the V(D)J sequence AIRR file\n",
    "- Output directory path to write the translated sequences\n",
    "- Reference IgBlast database to perform alignment and translation\n",
    "\n",
    "### Download example data and reference database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create tutorial directory and download example data\n",
    "mkdir -p tutorial\n",
    "wget -P tutorial https://zenodo.org/records/11373741/files/AIRR_subject1_FNA_d0_1_Y1.tsv\n",
    "\n",
    "# Download and extract IgBlast reference database\n",
    "wget -P tutorial -c https://github.com/nf-core/test-datasets/raw/airrflow/database-cache/igblast_base.zip\n",
    "unzip tutorial/igblast_base.zip -d tutorial\n",
    "rm tutorial/igblast_base.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the translation command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-08-20 13:40:26,220 - INFO - Converting AIRR table to FastA for IgBlast translation...\n",
      "2025-08-20 13:40:26,224 - INFO - Calling IgBlast for running translation...\n",
      "2025-08-20 13:40:27,860 - INFO - Saved the translations in the dataframe (sequence_aa contains the full translation and sequence_vdj_aa contains the VDJ translation).\n",
      "2025-08-20 13:40:27,864 - INFO - Took 1.64 seconds\n",
      "2025-08-20 13:40:27,864 - INFO - Saved the translations in ../tutorial/output/AIRR_subject1_FNA_d0_1_Y1_translated.tsv file.\n"
     ]
    }
   ],
   "source": [
    "! amulety translate-igblast ../tutorial/AIRR_subject1_FNA_d0_1_Y1.tsv ../tutorial/output ../tutorial/igblast_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding sequences\n",
    "\n",
    "Now we are ready to embed the sequences using various models. AMULETY uses a unified `embed` command that supports all available models.\n",
    "\n",
    "### Basic usage\n",
    "\n",
    "The basic syntax for the embed command is:\n",
    "\n",
    "```bash\n",
    "amulety embed --input-airr [INPUT_FILE] --chain [CHAIN] --model [MODEL] --batch-size [BATCH_SIZE] --output-file-path [OUTPUT]\n",
    "```\n",
    "\n",
    "### Required arguments:\n",
    "\n",
    "* `--chain`: Chain(s) to embed\n",
    "  - For BCR: `H` (Heavy), `L` (Light), `HL` (Heavy-Light pairs), `LH` (Light-Heavy pairs), `H+L` (Both chains separately)\n",
    "  - For TCR: `H` (Beta/Delta), `L` (Alpha/Gamma), `HL` (Beta-Alpha/Delta-Gamma pairs), `LH` (Alpha-Beta/Gamma-Delta pairs), `H+L` (Both chains separately)\n",
    "\n",
    "* `--model`: The embedding model to use (see model list above)\n",
    "\n",
    "* `--output-file-path`: Path to save embeddings (supports `.pt`, `.csv`, `.tsv` extensions)\n",
    "\n",
    "* `input_file`: Path to the input AIRR file\n",
    "\n",
    "### Optional arguments:\n",
    "\n",
    "* `--sequence-col`: Column containing amino acid sequences (default: `sequence_vdj_aa`)\n",
    "* `--cell-id-col`: Column containing single-cell barcodes (default: `cell_id`)\n",
    "* `--batch-size`: Mini-batch size for processing (default: 50)\n",
    "* `--cache-dir`: Directory for caching model weights (default: `/tmp/amulety`)\n",
    "* `--duplicate-col`: Column for selecting best chain when multiple exist (default: `duplicate_count`)\n",
    "\n",
    "### Custom model arguments (for `--model custom`):\n",
    "\n",
    "* `--model-path`: HuggingFace model name or local path\n",
    "* `--embedding-dimension`: Embedding dimension\n",
    "* `--max-length`: Maximum sequence length\n",
    "\n",
    "### Output formats:\n",
    "\n",
    "- `.pt` files: PyTorch tensors saved with `torch.save()` (embeddings only)\n",
    "- `.csv/.tsv` files: Include cell barcodes/sequence IDs as indices with embeddings\n",
    "\n",
    "The package automatically detects and uses GPU when available. Adjust `--batch-size` to avoid GPU out-of-memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCR embedding examples\n",
    "\n",
    "Let's demonstrate embedding BCR sequences using different models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AntiBERTy (BCR-specific model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-08-20 11:51:53,375 - INFO - Detected single-cell data format\n",
      "2025-08-20 11:51:53,376 - INFO - Processing single-cell data...\n",
      "2025-08-20 11:51:53,438 - INFO - AntiBERTy loaded. Size: 26.03 M\n",
      "2025-08-20 11:51:53,438 - INFO - Batch 1/48\n",
      "2025-08-20 11:51:53,475 - INFO - Batch 2/48\n",
      "2025-08-20 11:51:53,502 - INFO - Batch 3/48\n",
      "2025-08-20 11:51:53,527 - INFO - Batch 4/48\n",
      "2025-08-20 11:51:53,553 - INFO - Batch 5/48\n",
      "2025-08-20 11:51:53,579 - INFO - Batch 6/48\n",
      "2025-08-20 11:51:53,606 - INFO - Batch 7/48\n",
      "2025-08-20 11:51:53,633 - INFO - Batch 8/48\n",
      "2025-08-20 11:51:53,658 - INFO - Batch 9/48\n",
      "2025-08-20 11:51:53,683 - INFO - Batch 10/48\n",
      "2025-08-20 11:51:53,708 - INFO - Batch 11/48\n",
      "2025-08-20 11:51:53,734 - INFO - Batch 12/48\n",
      "2025-08-20 11:51:53,759 - INFO - Batch 13/48\n",
      "2025-08-20 11:51:53,787 - INFO - Batch 14/48\n",
      "2025-08-20 11:51:53,813 - INFO - Batch 15/48\n",
      "2025-08-20 11:51:53,839 - INFO - Batch 16/48\n",
      "2025-08-20 11:51:53,866 - INFO - Batch 17/48\n",
      "2025-08-20 11:51:53,893 - INFO - Batch 18/48\n",
      "2025-08-20 11:51:53,918 - INFO - Batch 19/48\n",
      "2025-08-20 11:51:53,944 - INFO - Batch 20/48\n",
      "2025-08-20 11:51:53,970 - INFO - Batch 21/48\n",
      "2025-08-20 11:51:53,995 - INFO - Batch 22/48\n",
      "2025-08-20 11:51:54,021 - INFO - Batch 23/48\n",
      "2025-08-20 11:51:54,046 - INFO - Batch 24/48\n",
      "2025-08-20 11:51:54,072 - INFO - Batch 25/48\n",
      "2025-08-20 11:51:54,098 - INFO - Batch 26/48\n",
      "2025-08-20 11:51:54,124 - INFO - Batch 27/48\n",
      "2025-08-20 11:51:54,156 - INFO - Batch 28/48\n",
      "2025-08-20 11:51:54,181 - INFO - Batch 29/48\n",
      "2025-08-20 11:51:54,207 - INFO - Batch 30/48\n",
      "2025-08-20 11:51:54,234 - INFO - Batch 31/48\n",
      "2025-08-20 11:51:54,260 - INFO - Batch 32/48\n",
      "2025-08-20 11:51:54,285 - INFO - Batch 33/48\n",
      "2025-08-20 11:51:54,310 - INFO - Batch 34/48\n",
      "2025-08-20 11:51:54,335 - INFO - Batch 35/48\n",
      "2025-08-20 11:51:54,360 - INFO - Batch 36/48\n",
      "2025-08-20 11:51:54,386 - INFO - Batch 37/48\n",
      "2025-08-20 11:51:54,411 - INFO - Batch 38/48\n",
      "2025-08-20 11:51:54,436 - INFO - Batch 39/48\n",
      "2025-08-20 11:51:54,462 - INFO - Batch 40/48\n",
      "2025-08-20 11:51:54,489 - INFO - Batch 41/48\n",
      "2025-08-20 11:51:54,514 - INFO - Batch 42/48\n",
      "2025-08-20 11:51:54,540 - INFO - Batch 43/48\n",
      "2025-08-20 11:51:54,567 - INFO - Batch 44/48\n",
      "2025-08-20 11:51:54,593 - INFO - Batch 45/48\n",
      "2025-08-20 11:51:54,619 - INFO - Batch 46/48\n",
      "2025-08-20 11:51:54,647 - INFO - Batch 47/48\n",
      "2025-08-20 11:51:54,676 - INFO - Batch 48/48\n",
      "2025-08-20 11:51:54,694 - INFO - Took 1.26 seconds\n",
      "2025-08-20 11:51:54,697 - INFO - Saved embedding at ../tutorial/test_embedding.pt\n"
     ]
    }
   ],
   "source": [
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model antiberty --batch-size 2 --output-file-path ../tutorial/test_embedding.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AntiBERTa2 (BCR-specific model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-08-21 11:55:10,483 - INFO - Detected single-cell data format\n",
      "2025-08-21 11:55:10,484 - INFO - Processing single-cell data...\n",
      "RoFormerForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "RoFormerForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "RoFormerForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "2025-08-21 11:55:10,970 - INFO - AntiBERTa2 loaded. Size: 202.642462 M\n",
      "2025-08-21 11:55:10,970 - INFO - Batch 1/48.\n",
      "2025-08-21 11:55:13,036 - INFO - Batch 2/48.\n",
      "2025-08-21 11:55:13,306 - INFO - Batch 3/48.\n",
      "2025-08-21 11:55:13,567 - INFO - Batch 4/48.\n",
      "2025-08-21 11:55:13,844 - INFO - Batch 5/48.\n",
      "2025-08-21 11:55:14,118 - INFO - Batch 6/48.\n",
      "2025-08-21 11:55:14,414 - INFO - Batch 7/48.\n",
      "2025-08-21 11:55:14,687 - INFO - Batch 8/48.\n",
      "2025-08-21 11:55:14,970 - INFO - Batch 9/48.\n",
      "2025-08-21 11:55:15,253 - INFO - Batch 10/48.\n",
      "2025-08-21 11:55:15,516 - INFO - Batch 11/48.\n",
      "2025-08-21 11:55:15,779 - INFO - Batch 12/48.\n",
      "2025-08-21 11:55:16,022 - INFO - Batch 13/48.\n",
      "2025-08-21 11:55:16,251 - INFO - Batch 14/48.\n",
      "2025-08-21 11:55:16,480 - INFO - Batch 15/48.\n",
      "2025-08-21 11:55:16,709 - INFO - Batch 16/48.\n",
      "2025-08-21 11:55:16,940 - INFO - Batch 17/48.\n",
      "2025-08-21 11:55:17,180 - INFO - Batch 18/48.\n",
      "2025-08-21 11:55:17,409 - INFO - Batch 19/48.\n",
      "2025-08-21 11:55:17,639 - INFO - Batch 20/48.\n",
      "2025-08-21 11:55:17,873 - INFO - Batch 21/48.\n",
      "2025-08-21 11:55:18,105 - INFO - Batch 22/48.\n",
      "2025-08-21 11:55:18,334 - INFO - Batch 23/48.\n",
      "2025-08-21 11:55:18,577 - INFO - Batch 24/48.\n",
      "2025-08-21 11:55:18,807 - INFO - Batch 25/48.\n",
      "2025-08-21 11:55:19,039 - INFO - Batch 26/48.\n",
      "2025-08-21 11:55:19,275 - INFO - Batch 27/48.\n",
      "2025-08-21 11:55:19,502 - INFO - Batch 28/48.\n",
      "2025-08-21 11:55:19,734 - INFO - Batch 29/48.\n",
      "2025-08-21 11:55:19,965 - INFO - Batch 30/48.\n",
      "2025-08-21 11:55:20,199 - INFO - Batch 31/48.\n",
      "2025-08-21 11:55:20,435 - INFO - Batch 32/48.\n",
      "2025-08-21 11:55:20,670 - INFO - Batch 33/48.\n",
      "2025-08-21 11:55:20,901 - INFO - Batch 34/48.\n",
      "2025-08-21 11:55:21,133 - INFO - Batch 35/48.\n",
      "2025-08-21 11:55:21,364 - INFO - Batch 36/48.\n",
      "2025-08-21 11:55:21,595 - INFO - Batch 37/48.\n",
      "2025-08-21 11:55:21,829 - INFO - Batch 38/48.\n",
      "2025-08-21 11:55:22,062 - INFO - Batch 39/48.\n",
      "2025-08-21 11:55:22,292 - INFO - Batch 40/48.\n",
      "2025-08-21 11:55:22,529 - INFO - Batch 41/48.\n",
      "2025-08-21 11:55:22,759 - INFO - Batch 42/48.\n",
      "2025-08-21 11:55:22,992 - INFO - Batch 43/48.\n",
      "2025-08-21 11:55:23,222 - INFO - Batch 44/48.\n",
      "2025-08-21 11:55:23,458 - INFO - Batch 45/48.\n",
      "2025-08-21 11:55:23,695 - INFO - Batch 46/48.\n",
      "2025-08-21 11:55:23,932 - INFO - Batch 47/48.\n",
      "2025-08-21 11:55:24,166 - INFO - Batch 48/48.\n",
      "2025-08-21 11:55:24,307 - INFO - Took 13.34 seconds\n",
      "2025-08-21 11:55:24,315 - INFO - Saved embedding at ../tutorial/AIRR_subject1_FNA_d0_1_Y1_antiberta2.pt\n"
     ]
    }
   ],
   "source": [
    "# Embed heavy-light chain pairs using AntiBERTa2\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model antiberta2 --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_antiberta2.pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AbLang (BCR-specific model with separate heavy/light models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed both heavy and light chains separately using AbLang\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H+L --model ablang --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_ablang.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BALM-paired model (BCR paired chains)\n",
    "\n",
    "BALM-paired is a specialized model for BCR heavy-light chain pairs. It automatically downloads the model weights when first used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-06 14:54:13--  https://zenodo.org/records/8237396/files/BALM-paired.tar.gz\n",
      "Resolving zenodo.org (zenodo.org)... 188.184.103.159, 188.184.98.238, 188.185.79.172, ...\n",
      "Connecting to zenodo.org (zenodo.org)|188.184.103.159|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1129993036 (1.1G) [application/octet-stream]\n",
      "Saving to: ‘tutorial/BALM-paired.tar.gz.1’\n",
      "\n",
      "BALM-paired.tar.gz. 100%[===================>]   1.05G  37.8MB/s    in 26s     \n",
      "\n",
      "2024-06-06 14:54:40 (41.3 MB/s) - ‘tutorial/BALM-paired.tar.gz.1’ saved [1129993036/1129993036]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget -P tutorial https://zenodo.org/records/8237396/files/BALM-paired.tar.gz\n",
    "tar -xzf tutorial/BALM-paired.tar.gz -C tutorial\n",
    "rm tutorial/BALM-paired.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the parameters mentioned above, we need to specify the following parameters:\n",
    "\n",
    "* `modelpath`: the path to the downloaded model weights\n",
    "\n",
    "* `embedding-dimension`: the dimension of the embedding\n",
    "\n",
    "* `max-length`: maximum length taken by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m1.0\u001b[0m\n",
      "\n",
      "2024-06-06 15:21:05,068 - INFO - Processing single-cell BCR data...\n",
      "2024-06-06 15:21:05,068 - INFO - Concatenating heavy and light chain per cell...\n",
      "2024-06-06 15:21:07,869 - INFO - Model size: 303.92M\n",
      "Batch 1/4\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/mw957/.conda/envs/bcrembed/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Batch 2/4\n",
      "\n",
      "Batch 3/4\n",
      "\n",
      "Batch 4/4\n",
      "\n",
      "2024-06-06 15:28:50,302 - INFO - Took 462.43 seconds\n",
      "2024-06-06 15:28:50,423 - INFO - Saved embedding at tutorial/AIRR_subject1_FNA_d0_1_Y1_BALM-paired.tsv\n"
     ]
    }
   ],
   "source": [
    "# Embed heavy-light chain pairs using BALM-paired\n",
    "# The model will be automatically downloaded on first use\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain HL --model balm-paired --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_balm_paired.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TCR embedding examples\n",
    "\n",
    "AMULETY also supports TCR-specific models. Let's demonstrate with some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCR-BERT (TCR-specific model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed TCR beta-alpha chain pairs using TCR-BERT\n",
    "# Note: This assumes you have TCR data in AIRR format\n",
    "! amulety embed --input-airr ../tutorial/tcr_data.tsv --chain HL --model tcr-bert --batch-size 2 --output-file-path ../tutorial/tcr_embeddings_tcrbert.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCRT5 (TCR beta chain only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed TCR beta chains using TCRT5 (only supports H/beta chains)\n",
    "! amulety embed --input-airr ../tutorial/tcr_data.tsv --chain H --model tcrt5 --batch-size 2 --output-file-path ../tutorial/tcr_embeddings_tcrt5.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESM2 (Protein language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed heavy chains only using ESM2\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model esm2 --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_esm2.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immune2Vec (Universal immune receptor model)\n",
    "\n",
    "Immune2Vec can be used for both BCR and TCR sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed sequences using Immune2Vec (works for both BCR and TCR)\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model immune2vec --batch-size 2 --output-file-path ../tutorial/immune2vec_embeddings.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom/Fine-tuned models\n",
    "\n",
    "You can use custom or fine-tuned models from HuggingFace or local paths using the `custom` model type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Using a fine-tuned ESM2 model from HuggingFace\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model custom \\\n",
    "  --model-path \"AmelieSchreiber/esm2_t6_8M_UR50D-finetuned-localization\" \\\n",
    "  --embedding-dimension 320 \\\n",
    "  --max-length 512 \\\n",
    "  --batch-size 2 \\\n",
    "  --output-file-path ../tutorial/custom_model_embeddings.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking dependencies\n",
    "\n",
    "Some models require additional dependencies that are not installed by default. You can check which dependencies are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Check which optional dependencies are missing\n",
    "amulety check-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced usage tips\n",
    "\n",
    "### Chain selection for multiple chains\n",
    "\n",
    "When your data contains multiple chains of the same type per cell, AMULETY uses the `duplicate_count` column by default to select the best chain. You can specify a custom column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Use a custom quality score column for chain selection\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain HL --model antiberta2 --duplicate-col quality_score --batch-size 2 --output-file-path ../tutorial/embeddings_custom_selection.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory management\n",
    "\n",
    "For large datasets or limited GPU memory, adjust the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Use smaller batch size for large models or limited memory\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain HL --model antiberta2 --batch-size 1 --output-file-path ../tutorial/embeddings_small_batch.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom sequence columns\n",
    "\n",
    "If your AIRR file uses different column names for sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-08-21 11:55:52,980 - INFO - Detected single-cell data format\n",
      "2025-08-21 11:55:52,980 - INFO - Processing single-cell data...\n",
      "2025-08-21 11:55:53,079 - INFO - AntiBERTy loaded. Size: 26.03 M\n",
      "2025-08-21 11:55:53,079 - INFO - Batch 1/48\n",
      "2025-08-21 11:55:53,121 - INFO - Batch 2/48\n",
      "2025-08-21 11:55:53,147 - INFO - Batch 3/48\n",
      "2025-08-21 11:55:53,174 - INFO - Batch 4/48\n",
      "2025-08-21 11:55:53,200 - INFO - Batch 5/48\n",
      "2025-08-21 11:55:53,229 - INFO - Batch 6/48\n",
      "2025-08-21 11:55:53,257 - INFO - Batch 7/48\n",
      "2025-08-21 11:55:53,287 - INFO - Batch 8/48\n",
      "2025-08-21 11:55:53,314 - INFO - Batch 9/48\n",
      "2025-08-21 11:55:53,344 - INFO - Batch 10/48\n",
      "2025-08-21 11:55:53,371 - INFO - Batch 11/48\n",
      "2025-08-21 11:55:53,401 - INFO - Batch 12/48\n",
      "2025-08-21 11:55:53,429 - INFO - Batch 13/48\n",
      "2025-08-21 11:55:53,457 - INFO - Batch 14/48\n",
      "2025-08-21 11:55:53,485 - INFO - Batch 15/48\n",
      "2025-08-21 11:55:53,514 - INFO - Batch 16/48\n",
      "2025-08-21 11:55:53,545 - INFO - Batch 17/48\n",
      "2025-08-21 11:55:53,575 - INFO - Batch 18/48\n",
      "2025-08-21 11:55:53,608 - INFO - Batch 19/48\n",
      "2025-08-21 11:55:53,638 - INFO - Batch 20/48\n",
      "2025-08-21 11:55:53,666 - INFO - Batch 21/48\n",
      "2025-08-21 11:55:53,694 - INFO - Batch 22/48\n",
      "2025-08-21 11:55:53,724 - INFO - Batch 23/48\n",
      "2025-08-21 11:55:53,753 - INFO - Batch 24/48\n",
      "2025-08-21 11:55:53,781 - INFO - Batch 25/48\n",
      "2025-08-21 11:55:53,810 - INFO - Batch 26/48\n",
      "2025-08-21 11:55:53,841 - INFO - Batch 27/48\n",
      "2025-08-21 11:55:53,874 - INFO - Batch 28/48\n",
      "2025-08-21 11:55:53,903 - INFO - Batch 29/48\n",
      "2025-08-21 11:55:53,933 - INFO - Batch 30/48\n",
      "2025-08-21 11:55:53,966 - INFO - Batch 31/48\n",
      "2025-08-21 11:55:53,999 - INFO - Batch 32/48\n",
      "2025-08-21 11:55:54,030 - INFO - Batch 33/48\n",
      "2025-08-21 11:55:54,062 - INFO - Batch 34/48\n",
      "2025-08-21 11:55:54,091 - INFO - Batch 35/48\n",
      "2025-08-21 11:55:54,122 - INFO - Batch 36/48\n",
      "2025-08-21 11:55:54,153 - INFO - Batch 37/48\n",
      "2025-08-21 11:55:54,186 - INFO - Batch 38/48\n",
      "2025-08-21 11:55:54,218 - INFO - Batch 39/48\n",
      "2025-08-21 11:55:54,249 - INFO - Batch 40/48\n",
      "2025-08-21 11:55:54,283 - INFO - Batch 41/48\n",
      "2025-08-21 11:55:54,314 - INFO - Batch 42/48\n",
      "2025-08-21 11:55:54,344 - INFO - Batch 43/48\n",
      "2025-08-21 11:55:54,371 - INFO - Batch 44/48\n",
      "2025-08-21 11:55:54,400 - INFO - Batch 45/48\n",
      "2025-08-21 11:55:54,428 - INFO - Batch 46/48\n",
      "2025-08-21 11:55:54,457 - INFO - Batch 47/48\n",
      "2025-08-21 11:55:54,486 - INFO - Batch 48/48\n",
      "2025-08-21 11:55:54,507 - INFO - Took 1.43 seconds\n",
      "2025-08-21 11:55:54,508 - INFO - Saved embedding at ../tutorial/embeddings_custom_col.pt\n"
     ]
    }
   ],
   "source": [
    "# Use custom sequence column name\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model antiberty --sequence-col sequence_aa --batch-size 2 --output-file-path ../tutorial/embeddings_custom_col.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world examples and advanced usage\n",
    "\n",
    "Let's demonstrate AMULETY with practical examples that mirror real research workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing IgBlast (Required for Translation)\n",
    "\n",
    "The `translate-igblast` command requires IgBlast to be installed. If you encounter the error `'igblastn' not found`, install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install IgBlast using conda (recommended)\n",
    "%conda install -c bioconda igblast -y\n",
    "\n",
    "# Verify installation\n",
    "!which igblastn\n",
    "\n",
    "# Test if IgBlast works\n",
    "!igblastn -help | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing optional dependencies\n",
    "\n",
    "Some advanced models require manual installation. Let's check what's missing and install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Check which dependencies are missing\n",
    "amulety check-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Immune2Vec (for universal immune receptor embeddings)\n",
    "\n",
    "Immune2Vec also requires manual installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install Immune2Vec\n",
    "git clone https://bitbucket.org/yaarilab/immune2vec_model.git\n",
    "cd immune2vec_model\n",
    "# The repository will be used by AMULETY automatically\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: BCR analysis workflow\n",
    "\n",
    "A typical BCR analysis workflow using different embedding approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample BCR data created with 5 sequences\n",
      "\n",
      "Data preview:\n",
      "  sequence_id                                    sequence_vdj_aa locus  \\\n",
      "0     BCR_001  EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYWMNWVRQAPGKGLE...   IGH   \n",
      "1     BCR_002  QSVLTQPPSVSAAPGQKVTISCSGSSSNIGNNYLSWYQQLPGTPPK...   IGL   \n",
      "2     BCR_003  QLQLQESGSGLVKPSQTLSLTCAVSGGSINSGDYSWSWIRQPPGKG...   IGH   \n",
      "3     BCR_004  DIQMTQSPSSLSASVGDRVTITCRASQGISNSLAWFQQKPGKAPKL...   IGK   \n",
      "4     BCR_005  EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYWMNWVRQAPGKGLE...   IGH   \n",
      "\n",
      "        v_call cell_id  duplicate_count  \n",
      "0  IGHV3-23*01  cell_1               23  \n",
      "1  IGLV2-14*01  cell_1              118  \n",
      "2  IGHV4-34*01  cell_2                6  \n",
      "3  IGKV1-39*01  cell_2               23  \n",
      "4  IGHV3-23*01  cell_3               15  \n",
      "\n",
      "Columns: ['sequence_id', 'sequence_vdj_aa', 'locus', 'v_call', 'cell_id', 'duplicate_count']\n",
      "\n",
      "Chain distribution:\n",
      "locus\n",
      "IGH    3\n",
      "IGL    1\n",
      "IGK    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample BCR data (based on real AIRR format)\n",
    "bcr_data = {\n",
    "    'sequence_id': ['BCR_001', 'BCR_002', 'BCR_003', 'BCR_004', 'BCR_005'],\n",
    "    'sequence_vdj_aa': [\n",
    "        'EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYWMNWVRQAPGKGLEWVANIKQDGTEKYYVDSVKGRFTISRDNAEDSVYLQMNSLRAEDTAVYYCARENLPSFFYYDSSAYLPEATFDFWGQGTMVTVSS',\n",
    "        'QSVLTQPPSVSAAPGQKVTISCSGSSSNIGNNYLSWYQQLPGTPPKLLIYENNQRPSGIPDRFSGSKSGTSATLDITGLQTGDEADYYCGTWDSSLSAGVFGGGTKLTVL',\n",
    "        'QLQLQESGSGLVKPSQTLSLTCAVSGGSINSGDYSWSWIRQPPGKGLEWIGSIYHSGSTSYNPSLKSRVTISVDRSKNQLSLKLSSATAADTAVYYCARSTVNIWGTFEYWGQGTLVTVSS',\n",
    "        'DIQMTQSPSSLSASVGDRVTITCRASQGISNSLAWFQQKPGKAPKLLLYTASRLESGVPSRFSGSGSGTDYTLTISSLQPEDFATYYCQQYYSSVMYTFGQGTKLEIK',\n",
    "        'EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYWMNWVRQAPGKGLEWVANIKQDGTEKYYVDSVKGRFTISRDNAEDSVYLQMNSLRAEDTAVYYCARENLPSFFYYDSSAYLPEATFDFWGQGTMVTVSS'\n",
    "    ],\n",
    "    'locus': ['IGH', 'IGL', 'IGH', 'IGK', 'IGH'],\n",
    "    'v_call': ['IGHV3-23*01', 'IGLV2-14*01', 'IGHV4-34*01', 'IGKV1-39*01', 'IGHV3-23*01'],  # Required by AMULETY\n",
    "    'cell_id': ['cell_1', 'cell_1', 'cell_2', 'cell_2', 'cell_3'],\n",
    "    'duplicate_count': [23, 118, 6, 23, 15]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "bcr_df = pd.DataFrame(bcr_data)\n",
    "bcr_df.to_csv('../tutorial/sample_bcr_data.tsv', sep='\\t', index=False)\n",
    "print(\"Sample BCR data created with\", len(bcr_df), \"sequences\")\n",
    "print(\"\\nData preview:\")\n",
    "print(bcr_df.head())\n",
    "print(\"\\nColumns:\", list(bcr_df.columns))\n",
    "print(\"\\nChain distribution:\")\n",
    "print(bcr_df['locus'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BCR Embedding Workflow ===\n",
      "1. Embedding with AntiBERTy (BCR-specific)...\n",
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-09-03 16:07:54,436 - INFO - Detected single-cell data format\n",
      "2025-09-03 16:07:54,436 - INFO - Single-cell AIRR data detected (all entries have cell_id).\n",
      "2025-09-03 16:07:54,437 - INFO - Removed 2 sequences not matching H chain\n",
      "2025-09-03 16:07:54,556 - INFO - AntiBERTy loaded. Size: 26.03 M\n",
      "2025-09-03 16:07:54,556 - INFO - Batch 1/2\n",
      "2025-09-03 16:07:54,618 - INFO - Batch 2/2\n",
      "2025-09-03 16:07:54,638 - INFO - Took 0.08 seconds\n",
      "2025-09-03 16:07:54,640 - INFO - Saved embedding at ../tutorial/bcr_antiberty.pt\n",
      "2. Embedding with AbLang (separate H/L models)...\n",
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-09-03 16:07:57,431 - INFO - Detected single-cell data format\n",
      "2025-09-03 16:07:57,432 - INFO - Single-cell AIRR data detected (all entries have cell_id).\n",
      "2025-09-03 16:07:59,116 - INFO - AbLang heavy chain model loaded\n",
      "2025-09-03 16:07:59,116 - INFO - Batch 1/3\n",
      "2025-09-03 16:07:59,217 - INFO - Batch 2/3\n",
      "2025-09-03 16:07:59,301 - INFO - Batch 3/3\n",
      "2025-09-03 16:07:59,350 - INFO - AbLang embedding completed. Took 0.23 seconds\n",
      "2025-09-03 16:07:59,363 - INFO - Saved embedding at ../tutorial/bcr_ablang.pt\n",
      "3. Embedding with ESM2 (protein language model)...\n",
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-09-03 16:08:02,225 - INFO - Detected single-cell data format\n",
      "2025-09-03 16:08:02,225 - INFO - Processing both BCR and TCR sequences from the file.\n",
      "2025-09-03 16:08:02,225 - INFO - Single-cell AIRR data detected (all entries have cell_id).\n",
      "2025-09-03 16:08:02,225 - INFO - Removed 2 sequences not matching H chain\n",
      "tokenizer_config.json: 100%|█████████████████| 95.0/95.0 [00:00<00:00, 87.1kB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 93.0/93.0 [00:00<00:00, 1.24MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 125/125 [00:00<00:00, 1.37MB/s]\n",
      "config.json: 100%|█████████████████████████████| 724/724 [00:00<00:00, 1.16MB/s]\n",
      "model.safetensors: 100%|████████████████████| 2.61G/2.61G [00:04<00:00, 597MB/s]\n",
      "Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForMaskedLM: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-09-03 16:08:08,114 - INFO - ESM2 650M model size: 651.04 M\n",
      "2025-09-03 16:08:08,115 - INFO - Batch 1/2.\n",
      "2025-09-03 16:08:14,223 - INFO - Batch 2/2.\n",
      "2025-09-03 16:08:15,032 - INFO - Took 6.92 seconds\n",
      "2025-09-03 16:08:15,053 - INFO - Saved embedding at ../tutorial/bcr_esm2.pt\n",
      "4. Embedding with BALM-paired (paired chains)...\n",
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-09-03 16:08:18,705 - INFO - Detected single-cell data format\n",
      "2025-09-03 16:08:18,706 - INFO - Single-cell AIRR data detected (all entries have cell_id).\n",
      "2025-09-03 16:09:00,795 - INFO - Model size: 303.92M\n",
      "Batch 1/1\n",
      "\n",
      "2025-09-03 16:09:01,373 - INFO - Took 0.58 seconds\n",
      "2025-09-03 16:09:01,381 - INFO - Saved embedding at ../tutorial/bcr_balm_paired.pt\n",
      "BCR embedding workflow completed!\n"
     ]
    }
   ],
   "source": [
    "# BCR embedding workflow - comparing different approaches\n",
    "\n",
    "! echo \"=== BCR Embedding Workflow ===\"\n",
    "\n",
    "# 1. BCR-specific model: AntiBERTy (heavy chains)\n",
    "! echo \"1. Embedding with AntiBERTy (BCR-specific)...\"\n",
    "! amulety embed --input-airr ../tutorial/sample_bcr_data.tsv --chain H --model antiberty --batch-size 2 --output-file-path ../tutorial/bcr_antiberty.pt\n",
    "\n",
    "# 2. BCR-specific model: AbLang (both heavy and light chains)\n",
    "! echo \"2. Embedding with AbLang (separate H/L models)...\"\n",
    "! amulety embed --input-airr ../tutorial/sample_bcr_data.tsv --chain H+L --model ablang --batch-size 2 --output-file-path ../tutorial/bcr_ablang.pt\n",
    "\n",
    "# 3. Protein language model: ESM2 (general protein model)\n",
    "! echo \"3. Embedding with ESM2 (protein language model)...\"\n",
    "! amulety embed --input-airr ../tutorial/sample_bcr_data.tsv --chain H --model esm2 --batch-size 2 --output-file-path ../tutorial/bcr_esm2.pt\n",
    "\n",
    "# 4. Paired chain model: BALM-paired (if available)\n",
    "! echo \"4. Embedding with BALM-paired (paired chains)...\"\n",
    "! amulety embed --input-airr ../tutorial/sample_bcr_data.tsv --chain HL --model balm-paired --batch-size 2 --output-file-path ../tutorial/bcr_balm_paired.pt\n",
    "\n",
    "! echo \"BCR embedding workflow completed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate BALM-paired Test\n",
    "\n",
    "Let's test BALM-paired separately to debug the tokenizer issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BALM-paired model separately...\n",
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-09-03 16:07:16,455 - INFO - Detected single-cell data format\n",
      "2025-09-03 16:07:16,456 - INFO - Single-cell AIRR data detected (all entries have cell_id).\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Separate BALM-paired test to debug tokenizer issue\n",
    "! echo \"Testing BALM-paired model separately...\"\n",
    "! amulety embed --input-airr ../tutorial/sample_bcr_data.tsv --chain HL --model balm-paired --batch-size 2 --output-file-path ../tutorial/bcr_balm_paired_test.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: TCR analysis workflow\n",
    "\n",
    "A comprehensive TCR analysis using different embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample TCR data created with 6 sequences\n",
      "\n",
      "Chain distribution:\n",
      "locus\n",
      "TRB    3\n",
      "TRA    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CDR3 length distribution:\n",
      "count     6.000000\n",
      "mean     13.833333\n",
      "std       1.940790\n",
      "min      11.000000\n",
      "25%      12.500000\n",
      "50%      14.500000\n",
      "75%      15.000000\n",
      "max      16.000000\n",
      "Name: cdr3_aa, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create sample TCR data (based on real test data)\n",
    "tcr_data = {\n",
    "    'sequence_id': ['TCR_001', 'TCR_002', 'TCR_003', 'TCR_004', 'TCR_005', 'TCR_006'],\n",
    "    'sequence_vdj_aa': [\n",
    "        'CASSLAPGATNEKLFF',  # TRB (beta chain)\n",
    "        'CAVNTGNQFYF',       # TRA (alpha chain)\n",
    "        'CASSLVGQGAYEQYF',   # TRB (beta chain)\n",
    "        'CAVRDMEYGNKLVF',    # TRA (alpha chain)\n",
    "        'CASSLPGQGAYEQYF',   # TRB (beta chain)\n",
    "        'CAVKDSNYQLIW'       # TRA (alpha chain)\n",
    "    ],\n",
    "    'cdr3_aa': [\n",
    "        'CASSLAPGATNEKLFF',\n",
    "        'CAVNTGNQFYF',\n",
    "        'CASSLVGQGAYEQYF',\n",
    "        'CAVRDMEYGNKLVF',\n",
    "        'CASSLPGQGAYEQYF',\n",
    "        'CAVKDSNYQLIW'\n",
    "    ],\n",
    "    'locus': ['TRB', 'TRA', 'TRB', 'TRA', 'TRB', 'TRA'],\n",
    "    'cell_id': ['cell_1', 'cell_1', 'cell_2', 'cell_2', 'cell_3', 'cell_3'],\n",
    "    'duplicate_count': [10, 8, 15, 12, 20, 18],\n",
    "    'v_call': ['TRBV7-9*01', 'TRAV8-4*01', 'TRBV7-2*01', 'TRAV13-1*01', 'TRBV7-2*01', 'TRAV12-1*01'],\n",
    "    'j_call': ['TRBJ2-1*01', 'TRAJ49*01', 'TRBJ2-7*01', 'TRAJ56*01', 'TRBJ2-7*01', 'TRAJ33*01']\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "tcr_df = pd.DataFrame(tcr_data)\n",
    "tcr_df.to_csv('../tutorial/sample_tcr_data.tsv', sep='\\t', index=False)\n",
    "print(\"Sample TCR data created with\", len(tcr_df), \"sequences\")\n",
    "print(\"\\nChain distribution:\")\n",
    "print(tcr_df['locus'].value_counts())\n",
    "print(\"\\nCDR3 length distribution:\")\n",
    "print(tcr_df['cdr3_aa'].str.len().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test: Verify Command Format\n",
    "\n",
    "Let's test the correct command format with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the correct command format\n",
    "!python3 -m amulety embed --help | head -10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amulety_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
