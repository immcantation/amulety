{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMULETY CLI Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to use AMULETY command line interface (CLI) to translate and embed both BCR (B-cell receptor) and TCR (T-cell receptor) sequences. AMULETY supports a wide range of embedding models for different immune receptor types.\n",
    "\n",
    "Before getting started, please install AMULETY using `pip install amulety`. You can check available commands from AMULETY by running the help command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mamulety [OPTIONS] COMMAND [ARGS]...\u001b[0m\u001b[1m                                    \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-install\u001b[0m\u001b[1;36m-completion\u001b[0m          Install completion for the current shell.      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-show\u001b[0m\u001b[1;36m-completion\u001b[0m             Show completion for the current shell, to copy \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                               it or customize the installation.              \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                        Show this message and exit.                    \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Commands \u001b[0m\u001b[2m──────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mtranslate-igblast \u001b[0m\u001b[1;36m \u001b[0m Translates nucleotide sequences to amino acid sequences  \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m                   \u001b[0m using IgBlast.                                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36membed             \u001b[0m\u001b[1;36m \u001b[0m Embeds sequences from an AIRR rearrangement file using   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m                   \u001b[0m the specified model. Example usage:                      \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mcheck-deps        \u001b[0m\u001b[1;36m \u001b[0m Check if optional embedding dependencies and tools are   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m                   \u001b[0m installed.                                               \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If AMULETY is installed via pip\n",
    "! amulety --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Commands\n",
    "\n",
    "AMULETY provides three main commands:\n",
    "\n",
    "1. **`translate-igblast`** - Translates nucleotide sequences to amino acid sequences using IgBlast\n",
    "2. **`embed`** - Embeds sequences using various models (BCR, TCR, and protein language models)\n",
    "3. **`check-deps`** - Check if optional embedding dependencies are installed\n",
    "\n",
    "### Supported Models\n",
    "\n",
    "AMULETY supports multiple categories of embedding models:\n",
    "\n",
    "**BCR Models:**\n",
    "- `ablang` - AbLang model for antibody sequences\n",
    "- `antiberta2` - AntiBERTa2 RoFormer model\n",
    "- `antiberty` - AntiBERTy model\n",
    "- `balm-paired` - BALM-paired model for heavy-light chain pairs\n",
    "\n",
    "**TCR Models:**\n",
    "- `tcr-bert` - TCR-BERT model for T-cell receptors\n",
    "- `tcremp` - TCREMP model for repertoire-level tasks\n",
    "- `tcrt5` - TCRT5 model (beta chains only)\n",
    "\n",
    "**Immune Models (BCR & TCR):**\n",
    "- `immune2vec` - Immune2Vec model for both BCR and TCR\n",
    "\n",
    "**Protein Language Models:**\n",
    "- `esm2` - ESM2 protein language model\n",
    "- `prott5` - ProtT5 protein language model\n",
    "- `custom` - Custom/fine-tuned models from HuggingFace\n",
    "\n",
    "### Chain Types\n",
    "\n",
    "AMULETY supports different chain input formats:\n",
    "- **H** - Heavy chain (BCR) or Beta/Delta chain (TCR)\n",
    "- **L** - Light chain (BCR) or Alpha/Gamma chain (TCR)\n",
    "- **HL** - Heavy-Light paired chains (BCR) or Beta-Alpha/Delta-Gamma paired chains (TCR)\n",
    "- **LH** - Light-Heavy paired chains (BCR) or Alpha-Beta/Gamma-Delta paired chains (TCR)\n",
    "- **H+L** - Both chains separately (not paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating nucleotides to amino acid sequences\n",
    "\n",
    "The inputs to the embedding models are [AIRR format files](https://docs.airr-community.org/en/stable/datarep/overview.html#datarepresentations) with immune receptor amino acid sequences. If the AIRR file only contains nucleotide sequences, the `amulety translate-igblast` command can help with the translation.\n",
    "\n",
    "### Requirements for translation:\n",
    "- Path to the V(D)J sequence AIRR file\n",
    "- Output directory path to write the translated sequences\n",
    "- Reference IgBlast database to perform alignment and translation\n",
    "\n",
    "### Download example data and reference database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create tutorial directory and download example data\n",
    "mkdir -p tutorial\n",
    "wget -P tutorial https://zenodo.org/records/11373741/files/AIRR_subject1_FNA_d0_1_Y1.tsv\n",
    "\n",
    "# Download and extract IgBlast reference database\n",
    "wget -P tutorial -c https://github.com/nf-core/test-datasets/raw/airrflow/database-cache/igblast_base.zip\n",
    "unzip tutorial/igblast_base.zip -d tutorial\n",
    "rm tutorial/igblast_base.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the translation command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-08-20 13:40:26,220 - INFO - Converting AIRR table to FastA for IgBlast translation...\n",
      "2025-08-20 13:40:26,224 - INFO - Calling IgBlast for running translation...\n",
      "2025-08-20 13:40:27,860 - INFO - Saved the translations in the dataframe (sequence_aa contains the full translation and sequence_vdj_aa contains the VDJ translation).\n",
      "2025-08-20 13:40:27,864 - INFO - Took 1.64 seconds\n",
      "2025-08-20 13:40:27,864 - INFO - Saved the translations in ../tutorial/output/AIRR_subject1_FNA_d0_1_Y1_translated.tsv file.\n"
     ]
    }
   ],
   "source": [
    "! amulety translate-igblast ../tutorial/AIRR_subject1_FNA_d0_1_Y1.tsv ../tutorial/output ../tutorial/igblast_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding sequences\n",
    "\n",
    "Now we are ready to embed the sequences using various models. AMULETY uses a unified `embed` command that supports all available models.\n",
    "\n",
    "### Basic usage\n",
    "\n",
    "The basic syntax for the embed command is:\n",
    "\n",
    "```bash\n",
    "amulety embed --input-airr [INPUT_FILE] --chain [CHAIN] --model [MODEL] --batch-size [BATCH_SIZE] --output-file-path [OUTPUT]\n",
    "```\n",
    "\n",
    "### Required arguments:\n",
    "\n",
    "* `--chain`: Chain(s) to embed\n",
    "  - For BCR: `H` (Heavy), `L` (Light), `HL` (Heavy-Light pairs), `LH` (Light-Heavy pairs), `H+L` (Both chains separately)\n",
    "  - For TCR: `H` (Beta/Delta), `L` (Alpha/Gamma), `HL` (Beta-Alpha/Delta-Gamma pairs), `LH` (Alpha-Beta/Gamma-Delta pairs), `H+L` (Both chains separately)\n",
    "\n",
    "* `--model`: The embedding model to use (see model list above)\n",
    "\n",
    "* `--output-file-path`: Path to save embeddings (supports `.pt`, `.csv`, `.tsv` extensions)\n",
    "\n",
    "* `input_file`: Path to the input AIRR file\n",
    "\n",
    "### Optional arguments:\n",
    "\n",
    "* `--sequence-col`: Column containing amino acid sequences (default: `sequence_vdj_aa`)\n",
    "* `--cell-id-col`: Column containing single-cell barcodes (default: `cell_id`)\n",
    "* `--batch-size`: Mini-batch size for processing (default: 50)\n",
    "* `--cache-dir`: Directory for caching model weights (default: `/tmp/amulety`)\n",
    "* `--duplicate-col`: Column for selecting best chain when multiple exist (default: `duplicate_count`)\n",
    "\n",
    "### Custom model arguments (for `--model custom`):\n",
    "\n",
    "* `--model-path`: HuggingFace model name or local path\n",
    "* `--embedding-dimension`: Embedding dimension\n",
    "* `--max-length`: Maximum sequence length\n",
    "\n",
    "### Output formats:\n",
    "\n",
    "- `.pt` files: PyTorch tensors saved with `torch.save()` (embeddings only)\n",
    "- `.csv/.tsv` files: Include cell barcodes/sequence IDs as indices with embeddings\n",
    "\n",
    "The package automatically detects and uses GPU when available. Adjust `--batch-size` to avoid GPU out-of-memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCR embedding examples\n",
    "\n",
    "Let's demonstrate embedding BCR sequences using different models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AntiBERTy (BCR-specific model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "2025-08-20 11:51:53,375 - INFO - Detected single-cell data format\n",
      "2025-08-20 11:51:53,376 - INFO - Processing single-cell data...\n",
      "2025-08-20 11:51:53,438 - INFO - AntiBERTy loaded. Size: 26.03 M\n",
      "2025-08-20 11:51:53,438 - INFO - Batch 1/48\n",
      "2025-08-20 11:51:53,475 - INFO - Batch 2/48\n",
      "2025-08-20 11:51:53,502 - INFO - Batch 3/48\n",
      "2025-08-20 11:51:53,527 - INFO - Batch 4/48\n",
      "2025-08-20 11:51:53,553 - INFO - Batch 5/48\n",
      "2025-08-20 11:51:53,579 - INFO - Batch 6/48\n",
      "2025-08-20 11:51:53,606 - INFO - Batch 7/48\n",
      "2025-08-20 11:51:53,633 - INFO - Batch 8/48\n",
      "2025-08-20 11:51:53,658 - INFO - Batch 9/48\n",
      "2025-08-20 11:51:53,683 - INFO - Batch 10/48\n",
      "2025-08-20 11:51:53,708 - INFO - Batch 11/48\n",
      "2025-08-20 11:51:53,734 - INFO - Batch 12/48\n",
      "2025-08-20 11:51:53,759 - INFO - Batch 13/48\n",
      "2025-08-20 11:51:53,787 - INFO - Batch 14/48\n",
      "2025-08-20 11:51:53,813 - INFO - Batch 15/48\n",
      "2025-08-20 11:51:53,839 - INFO - Batch 16/48\n",
      "2025-08-20 11:51:53,866 - INFO - Batch 17/48\n",
      "2025-08-20 11:51:53,893 - INFO - Batch 18/48\n",
      "2025-08-20 11:51:53,918 - INFO - Batch 19/48\n",
      "2025-08-20 11:51:53,944 - INFO - Batch 20/48\n",
      "2025-08-20 11:51:53,970 - INFO - Batch 21/48\n",
      "2025-08-20 11:51:53,995 - INFO - Batch 22/48\n",
      "2025-08-20 11:51:54,021 - INFO - Batch 23/48\n",
      "2025-08-20 11:51:54,046 - INFO - Batch 24/48\n",
      "2025-08-20 11:51:54,072 - INFO - Batch 25/48\n",
      "2025-08-20 11:51:54,098 - INFO - Batch 26/48\n",
      "2025-08-20 11:51:54,124 - INFO - Batch 27/48\n",
      "2025-08-20 11:51:54,156 - INFO - Batch 28/48\n",
      "2025-08-20 11:51:54,181 - INFO - Batch 29/48\n",
      "2025-08-20 11:51:54,207 - INFO - Batch 30/48\n",
      "2025-08-20 11:51:54,234 - INFO - Batch 31/48\n",
      "2025-08-20 11:51:54,260 - INFO - Batch 32/48\n",
      "2025-08-20 11:51:54,285 - INFO - Batch 33/48\n",
      "2025-08-20 11:51:54,310 - INFO - Batch 34/48\n",
      "2025-08-20 11:51:54,335 - INFO - Batch 35/48\n",
      "2025-08-20 11:51:54,360 - INFO - Batch 36/48\n",
      "2025-08-20 11:51:54,386 - INFO - Batch 37/48\n",
      "2025-08-20 11:51:54,411 - INFO - Batch 38/48\n",
      "2025-08-20 11:51:54,436 - INFO - Batch 39/48\n",
      "2025-08-20 11:51:54,462 - INFO - Batch 40/48\n",
      "2025-08-20 11:51:54,489 - INFO - Batch 41/48\n",
      "2025-08-20 11:51:54,514 - INFO - Batch 42/48\n",
      "2025-08-20 11:51:54,540 - INFO - Batch 43/48\n",
      "2025-08-20 11:51:54,567 - INFO - Batch 44/48\n",
      "2025-08-20 11:51:54,593 - INFO - Batch 45/48\n",
      "2025-08-20 11:51:54,619 - INFO - Batch 46/48\n",
      "2025-08-20 11:51:54,647 - INFO - Batch 47/48\n",
      "2025-08-20 11:51:54,676 - INFO - Batch 48/48\n",
      "2025-08-20 11:51:54,694 - INFO - Took 1.26 seconds\n",
      "2025-08-20 11:51:54,697 - INFO - Saved embedding at ../tutorial/test_embedding.pt\n"
     ]
    }
   ],
   "source": [
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model antiberty --batch-size 2 --output-file-path ../tutorial/test_embedding.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AntiBERTa2 (BCR-specific model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m0.1\u001b[0m.\u001b[1;36m1\u001b[0m\n",
      "\n",
      "\u001b[33mUsage: \u001b[0mamulety embed [OPTIONS]\n",
      "\u001b[2mTry \u001b[0m\u001b[2;34m'amulety embed \u001b[0m\u001b[1;2;34m-\u001b[0m\u001b[1;2;34m-help\u001b[0m\u001b[2;34m'\u001b[0m\u001b[2m for help.\u001b[0m\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m Error \u001b[0m\u001b[31m─────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m Missing option '\u001b[1;36m-\u001b[0m\u001b[1;36m-input\u001b[0m\u001b[1;36m-airr\u001b[0m' (env var: 'None').                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Embed heavy-light chain pairs using AntiBERTa2\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain HL --model antiberta2 --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_antiberta2.pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AbLang (BCR-specific model with separate heavy/light models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed both heavy and light chains separately using AbLang\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H+L --model ablang --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_ablang.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESM2 (Protein language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed heavy chains only using ESM2\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model esm2 --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_esm2.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TCR embedding examples\n",
    "\n",
    "AMULETY also supports TCR-specific models. Let's demonstrate with some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCR-BERT (TCR-specific model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed TCR beta-alpha chain pairs using TCR-BERT\n",
    "# Note: This assumes you have TCR data in AIRR format\n",
    "! amulety embed --input-airr ../tutorial/tcr_data.tsv --chain HL --model tcr-bert --batch-size 2 --output-file-path ../tutorial/tcr_embeddings_tcrbert.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCRT5 (TCR beta chain only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed TCR beta chains using TCRT5 (only supports H/beta chains)\n",
    "! amulety embed --input-airr ../tutorial/tcr_data.tsv --chain H --model tcrt5 --batch-size 2 --output-file-path ../tutorial/tcr_embeddings_tcrt5.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCREMP (TCR repertoire-level model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed TCR sequences using TCREMP (may require --skip-clustering for stability)\n",
    "! amulety embed --input-airr ../tutorial/tcr_data.tsv --chain H --model tcremp --skip-clustering --batch-size 2 --output-file-path ../tutorial/tcr_embeddings_tcremp.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BALM-paired model (BCR paired chains)\n",
    "\n",
    "BALM-paired is a specialized model for BCR heavy-light chain pairs. It automatically downloads the model weights when first used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-06 14:54:13--  https://zenodo.org/records/8237396/files/BALM-paired.tar.gz\n",
      "Resolving zenodo.org (zenodo.org)... 188.184.103.159, 188.184.98.238, 188.185.79.172, ...\n",
      "Connecting to zenodo.org (zenodo.org)|188.184.103.159|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1129993036 (1.1G) [application/octet-stream]\n",
      "Saving to: ‘tutorial/BALM-paired.tar.gz.1’\n",
      "\n",
      "BALM-paired.tar.gz. 100%[===================>]   1.05G  37.8MB/s    in 26s     \n",
      "\n",
      "2024-06-06 14:54:40 (41.3 MB/s) - ‘tutorial/BALM-paired.tar.gz.1’ saved [1129993036/1129993036]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget -P tutorial https://zenodo.org/records/8237396/files/BALM-paired.tar.gz\n",
    "tar -xzf tutorial/BALM-paired.tar.gz -C tutorial\n",
    "rm tutorial/BALM-paired.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the parameters mentioned above, we need to specify the following parameters:\n",
    "\n",
    "* `modelpath`: the path to the downloaded model weights\n",
    "\n",
    "* `embedding-dimension`: the dimension of the embedding\n",
    "\n",
    "* `max-length`: maximum length taken by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " █████  ███    ███ ██    ██ ██      ███████ ████████     ██    ██\n",
      "██   ██ ████  ████ ██    ██ ██      ██         ██         ██  ██\n",
      "███████ ██ ████ ██ ██    ██ ██      █████      ██          ████\n",
      "██   ██ ██  ██  ██ ██    ██ ██      ██         ██           ██\n",
      "██   ██ ██      ██  ██████  ███████ ███████    ██           ██\n",
      "\n",
      "AMULETY: Adaptive imMUne receptor Language model Embedding Tool\n",
      " version \u001b[1;36m1.0\u001b[0m\n",
      "\n",
      "2024-06-06 15:21:05,068 - INFO - Processing single-cell BCR data...\n",
      "2024-06-06 15:21:05,068 - INFO - Concatenating heavy and light chain per cell...\n",
      "2024-06-06 15:21:07,869 - INFO - Model size: 303.92M\n",
      "Batch 1/4\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/mw957/.conda/envs/bcrembed/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Batch 2/4\n",
      "\n",
      "Batch 3/4\n",
      "\n",
      "Batch 4/4\n",
      "\n",
      "2024-06-06 15:28:50,302 - INFO - Took 462.43 seconds\n",
      "2024-06-06 15:28:50,423 - INFO - Saved embedding at tutorial/AIRR_subject1_FNA_d0_1_Y1_BALM-paired.tsv\n"
     ]
    }
   ],
   "source": [
    "# Embed heavy-light chain pairs using BALM-paired\n",
    "# The model will be automatically downloaded on first use\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain HL --model balm-paired --batch-size 2 --output-file-path ../tutorial/AIRR_subject1_FNA_d0_1_Y1_balm_paired.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom/Fine-tuned models\n",
    "\n",
    "You can use custom or fine-tuned models from HuggingFace or local paths using the `custom` model type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Using a fine-tuned ESM2 model from HuggingFace\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model custom \\\n",
    "  --model-path \"AmelieSchreiber/esm2_t6_8M_UR50D-finetuned-localization\" \\\n",
    "  --embedding-dimension 320 \\\n",
    "  --max-length 512 \\\n",
    "  --batch-size 2 \\\n",
    "  --output-file-path ../tutorial/custom_model_embeddings.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immune2Vec (Universal immune receptor model)\n",
    "\n",
    "Immune2Vec can be used for both BCR and TCR sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Embed sequences using Immune2Vec (works for both BCR and TCR)\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model immune2vec --batch-size 2 --output-file-path ../tutorial/immune2vec_embeddings.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking dependencies\n",
    "\n",
    "Some models require additional dependencies that are not installed by default. You can check which dependencies are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Check which optional dependencies are missing\n",
    "amulety check-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced usage tips\n",
    "\n",
    "### Chain selection for multiple chains\n",
    "\n",
    "When your data contains multiple chains of the same type per cell, AMULETY uses the `duplicate_count` column by default to select the best chain. You can specify a custom column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Use a custom quality score column for chain selection\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain HL --model antiberta2 --duplicate-col quality_score --batch-size 2 --output-file-path ../tutorial/embeddings_custom_selection.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory management\n",
    "\n",
    "For large datasets or limited GPU memory, adjust the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Use smaller batch size for large models or limited memory\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain HL --model antiberta2 --batch-size 1 --output-file-path ../tutorial/embeddings_small_batch.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom sequence columns\n",
    "\n",
    "If your AIRR file uses different column names for sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Use custom sequence column name\n",
    "! amulety embed --input-airr ../tutorial/AIRR_subject1_FNA_d0_1_Y1_translated.tsv --chain H --model antiberty --sequence-col sequence_aa --batch-size 2 --output-file-path ../tutorial/embeddings_custom_col.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world examples and advanced usage\n",
    "\n",
    "Let's demonstrate AMULETY with practical examples that mirror real research workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing IgBlast (Required for Translation)\n",
    "\n",
    "The `translate-igblast` command requires IgBlast to be installed. If you encounter the error `'igblastn' not found`, install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install IgBlast using conda (recommended)\n",
    "%conda install -c bioconda igblast -y\n",
    "\n",
    "# Verify installation\n",
    "!which igblastn\n",
    "\n",
    "# Test if IgBlast works\n",
    "!igblastn -help | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing optional dependencies\n",
    "\n",
    "Some advanced models require manual installation. Let's check what's missing and install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Check which dependencies are missing\n",
    "amulety check-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing TCREMP (for TCR repertoire analysis)\n",
    "\n",
    "TCREMP requires manual installation from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install TCREMP (requires Python 3.11+)\n",
    "git clone https://github.com/antigenomics/tcremp.git\n",
    "cd tcremp\n",
    "pip install .\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Immune2Vec (for universal immune receptor embeddings)\n",
    "\n",
    "Immune2Vec also requires manual installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install Immune2Vec\n",
    "git clone https://bitbucket.org/yaarilab/immune2vec_model.git\n",
    "cd immune2vec_model\n",
    "# The repository will be used by AMULETY automatically\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: BCR analysis workflow\n",
    "\n",
    "A typical BCR analysis workflow using different embedding approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample BCR data created with 5 sequences\n",
      "\n",
      "Data preview:\n",
      "  sequence_id locus cell_id\n",
      "0     BCR_001   IGH  cell_1\n",
      "1     BCR_002   IGL  cell_1\n",
      "2     BCR_003   IGH  cell_2\n",
      "3     BCR_004   IGK  cell_2\n",
      "4     BCR_005   IGH  cell_3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample BCR data (based on real AIRR format)\n",
    "bcr_data = {\n",
    "    'sequence_id': ['BCR_001', 'BCR_002', 'BCR_003', 'BCR_004', 'BCR_005'],\n",
    "    'sequence_vdj_aa': [\n",
    "        'EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYWMNWVRQAPGKGLEWVANIKQDGTEKYYVDSVKGRFTISRDNAEDSVYLQMNSLRAEDTAVYYCARENLPSFFYYDSSAYLPEATFDFWGQGTMVTVSS',\n",
    "        'QSVLTQPPSVSAAPGQKVTISCSGSSSNIGNNYLSWYQQLPGTPPKLLIYENNQRPSGIPDRFSGSKSGTSATLDITGLQTGDEADYYCGTWDSSLSAGVFGGGTKLTVL',\n",
    "        'QLQLQESGSGLVKPSQTLSLTCAVSGGSINSGDYSWSWIRQPPGKGLEWIGSIYHSGSTSYNPSLKSRVTISVDRSKNQLSLKLSSATAADTAVYYCARSTVNIWGTFEYWGQGTLVTVSS',\n",
    "        'DIQMTQSPSSLSASVGDRVTITCRASQGISNSLAWFQQKPGKAPKLLLYTASRLESGVPSRFSGSGSGTDYTLTISSLQPEDFATYYCQQYYSSVMYTFGQGTKLEIK',\n",
    "        'EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYWMNWVRQAPGKGLEWVANIKQDGTEKYYVDSVKGRFTISRDNAEDSVYLQMNSLRAEDTAVYYCARENLPSFFYYDSSAYLPEATFDFWGQGTMVTVSS'\n",
    "    ],\n",
    "    'locus': ['IGH', 'IGL', 'IGH', 'IGK', 'IGH'],\n",
    "    'cell_id': ['cell_1', 'cell_1', 'cell_2', 'cell_2', 'cell_3'],\n",
    "    'duplicate_count': [23, 118, 6, 23, 15],\n",
    "    'v_call': ['IGHV3-7*01', 'IGLV1-51*02', 'IGHV4-30-2*01', 'IGKV1-NL1*01', 'IGHV3-7*01'],\n",
    "    'j_call': ['IGHJ3*01', 'IGLJ3*02', 'IGHJ4*02', 'IGKJ2*01', 'IGHJ3*01']\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "bcr_df = pd.DataFrame(bcr_data)\n",
    "bcr_df.to_csv('../tutorial/sample_bcr_data.tsv', sep='\\t', index=False)\n",
    "print(\"Sample BCR data created with\", len(bcr_df), \"sequences\")\n",
    "print(\"\\nData preview:\")\n",
    "print(bcr_df[['sequence_id', 'locus', 'cell_id']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# BCR embedding workflow - comparing different approaches\n",
    "\n",
    "echo \"=== BCR Embedding Workflow ===\"\n",
    "\n",
    "# 1. BCR-specific model: AntiBERTy (heavy chains)\n",
    "echo \"1. Embedding with AntiBERTy (BCR-specific)...\"\n",
    "! amulety embed --input-airr ../tutorial/sample_bcr_data.tsv --chain H --model antiberty --batch-size 2 --output-file-path ../tutorial/bcr_antiberty.pt\n",
    "\n",
    "# 2. BCR-specific model: AbLang (both heavy and light chains)\n",
    "echo \"2. Embedding with AbLang (separate H/L models)...\"\n",
    "! amulety embed --input-airr ../tutorial/sample_bcr_data.tsv --chain H+L --model ablang --batch-size 2 --output-file-path ../tutorial/bcr_ablang.pt\n",
    "\n",
    "# 3. Protein language model: ESM2 (general protein model)\n",
    "echo \"3. Embedding with ESM2 (protein language model)...\"\n",
    "! amulety embed --input-airr ../tutorial/sample_bcr_data.tsv --chain H --model esm2 --batch-size 2 --output-file-path ../tutorial/bcr_esm2.pt\n",
    "\n",
    "# 4. Paired chain model: BALM-paired (if available)\n",
    "echo \"4. Embedding with BALM-paired (paired chains)...\"\n",
    "! amulety embed --input-airr ../tutorial/sample_bcr_data.tsv --chain HL --model balm-paired --batch-size 2 --output-file-path ../tutorial/bcr_balm_paired.pt\n",
    "\n",
    "echo \"BCR embedding workflow completed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: TCR analysis workflow\n",
    "\n",
    "A comprehensive TCR analysis using different embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m      2\u001b[39m tcr_data = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msequence_id\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mTCR_001\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTCR_002\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTCR_003\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTCR_004\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTCR_005\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTCR_006\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msequence_vdj_aa\u001b[39m\u001b[33m'\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mj_call\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mTRBJ2-1*01\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTRAJ49*01\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTRBJ2-7*01\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTRAJ56*01\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTRBJ2-7*01\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTRAJ33*01\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     25\u001b[39m }\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Save to file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m tcr_df = \u001b[43mpd\u001b[49m.DataFrame(tcr_data)\n\u001b[32m     29\u001b[39m tcr_df.to_csv(\u001b[33m'\u001b[39m\u001b[33mtutorial/sample_tcr_data.tsv\u001b[39m\u001b[33m'\u001b[39m, sep=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample TCR data created with\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tcr_df), \u001b[33m\"\u001b[39m\u001b[33msequences\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Create sample TCR data (based on real test data)\n",
    "tcr_data = {\n",
    "    'sequence_id': ['TCR_001', 'TCR_002', 'TCR_003', 'TCR_004', 'TCR_005', 'TCR_006'],\n",
    "    'sequence_vdj_aa': [\n",
    "        'CASSLAPGATNEKLFF',  # TRB (beta chain)\n",
    "        'CAVNTGNQFYF',       # TRA (alpha chain)\n",
    "        'CASSLVGQGAYEQYF',   # TRB (beta chain)\n",
    "        'CAVRDMEYGNKLVF',    # TRA (alpha chain)\n",
    "        'CASSLPGQGAYEQYF',   # TRB (beta chain)\n",
    "        'CAVKDSNYQLIW'       # TRA (alpha chain)\n",
    "    ],\n",
    "    'cdr3_aa': [\n",
    "        'CASSLAPGATNEKLFF',\n",
    "        'CAVNTGNQFYF',\n",
    "        'CASSLVGQGAYEQYF',\n",
    "        'CAVRDMEYGNKLVF',\n",
    "        'CASSLPGQGAYEQYF',\n",
    "        'CAVKDSNYQLIW'\n",
    "    ],\n",
    "    'locus': ['TRB', 'TRA', 'TRB', 'TRA', 'TRB', 'TRA'],\n",
    "    'cell_id': ['cell_1', 'cell_1', 'cell_2', 'cell_2', 'cell_3', 'cell_3'],\n",
    "    'duplicate_count': [10, 8, 15, 12, 20, 18],\n",
    "    'v_call': ['TRBV7-9*01', 'TRAV8-4*01', 'TRBV7-2*01', 'TRAV13-1*01', 'TRBV7-2*01', 'TRAV12-1*01'],\n",
    "    'j_call': ['TRBJ2-1*01', 'TRAJ49*01', 'TRBJ2-7*01', 'TRAJ56*01', 'TRBJ2-7*01', 'TRAJ33*01']\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "tcr_df = pd.DataFrame(tcr_data)\n",
    "tcr_df.to_csv('tutorial/sample_tcr_data.tsv', sep='\\t', index=False)\n",
    "print(\"Sample TCR data created with\", len(tcr_df), \"sequences\")\n",
    "print(\"\\nChain distribution:\")\n",
    "print(tcr_df['locus'].value_counts())\n",
    "print(\"\\nCDR3 length distribution:\")\n",
    "print(tcr_df['cdr3_aa'].str.len().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test: Verify Command Format\n",
    "\n",
    "Let's test the correct command format with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the correct command format\n",
    "!python3 -m amulety embed --help | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with a small dataset (if available)\n",
    "# This shows the correct parameter order: --input-airr comes first\n",
    "# ! python3 -m amulety embed --input-airr ../tutorial/sample_bcr_data.tsv --chain H --model antiberty --batch-size 2 --output-file-path ../tutorial/test_embedding.pt\n",
    "\n",
    "print(\"✅ Tutorial commands have been updated with correct --input-airr parameter format!\")\n",
    "print(\"\\nCorrect format:\")\n",
    "print(\"amulety embed --input-airr <input_file> --chain <chain> --model <model> --output-file-path <output_file>\")\n",
    "print(\"\\nIncorrect format (old):\")\n",
    "print(\"amulety embed --chain <chain> --model <model> --output-file-path <output_file> <input_file>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amulety_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
